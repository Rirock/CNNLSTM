{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 1216, 192)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 从磁盘加载 NumPy 数组\n",
    "data_array = np.load('PBS45p_q10.npy')  \n",
    "data_array = data_array.astype('float32')\n",
    "data_array = data_array.transpose(0 ,2, 1)\n",
    "print(data_array.shape)  # (5001, 192, 1216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1216, 192)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 从磁盘加载 NumPy 数组\n",
    "label_array = np.load('PBS45p_q10_5000_SVD_60_80dB_ensemble1_Acq_Data.npy') \n",
    "label_array = label_array.astype('float32') \n",
    "label_array = label_array.transpose(0, 2, 1)\n",
    "print(label_array.shape)  # (5000, 192, 1216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 加载数据集（CNN+LSTM） 连续10帧\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader  #, TensorDataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_array, label_array, length=10):\n",
    "        self.length = length\n",
    "        self.data = torch.from_numpy(data_array).float().unsqueeze(1)#.to(device)\n",
    "        self.label = torch.from_numpy(label_array).float().unsqueeze(1)#.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx:idx+self.length], self.label[idx:idx+self.length]\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "data_num, height, width = label_array.shape\n",
    "seq_train, seq_val, seq_test = int(data_num*0.7), int(data_num*0.1), int(data_num*0.2)\n",
    "\n",
    "length = 10\n",
    "# data_tensor = torch.from_numpy(data_array)#.half()\n",
    "# label_tensor = torch.from_numpy(label_array)#.half()\n",
    "# data_tensor = data_tensor.unsqueeze(1).to(device)\n",
    "# label_tensor = label_tensor.unsqueeze(1).to(device)\n",
    "train_dataset = CustomDataset(data_array[:seq_train], label_array[:seq_train], length)\n",
    "val_dataset = CustomDataset(data_array[seq_train:seq_train+seq_val], label_array[seq_train:seq_train+seq_val], length)\n",
    "test_dataset = CustomDataset(data_array[seq_train+seq_val:seq_train+seq_val+seq_test], label_array[seq_train+seq_val:seq_train+seq_val+seq_test], length)\n",
    "\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTM(\n",
      "  (conv_lstm): ConvLSTM(\n",
      "    (cells): ModuleList(\n",
      "      (0): ConvLSTMCell(\n",
      "        (conv): Conv2d(17, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): CNNDecoder(\n",
      "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 10, 1, 1500, 121])\n"
     ]
    }
   ],
   "source": [
    "### 加载模型 CNN+LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_channels + hidden_channels, 4 * hidden_channels, kernel_size, padding=self.padding)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_cur, c_cur = hidden\n",
    "        combined = torch.cat([x, h_cur], dim=1)\n",
    "        conv_output = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, num_layers):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.cells = nn.ModuleList([ConvLSTMCell(input_channels if i == 0 else hidden_channels,\n",
    "                                                 hidden_channels, kernel_size) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        hidden = [(torch.zeros(batch_size, self.hidden_channels, height, width).to(x.device),\n",
    "                   torch.zeros(batch_size, self.hidden_channels, height, width).to(x.device)) for _ in range(self.num_layers)]\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :, :, :]\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                hidden[i] = cell(x_t, hidden[i])\n",
    "                x_t = hidden[i][0]\n",
    "            outputs.append(x_t.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(input_channels)\n",
    "        self.conv2 = nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.conv_lstm = ConvLSTM(input_channels=1, hidden_channels=16, kernel_size=3, num_layers=1)\n",
    "        self.decoder = CNNDecoder(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out = self.conv_lstm(x)\n",
    "        batch_size, seq_len, channels, height, width = lstm_out.size()\n",
    "        decoded_frames = []\n",
    "        for t in range(seq_len):\n",
    "            decoded_frame = self.decoder(lstm_out[:, t, :, :, :])\n",
    "            decoded_frames.append(decoded_frame.unsqueeze(1))\n",
    "        decoded_frames = torch.cat(decoded_frames, dim=1)\n",
    "        return decoded_frames\n",
    "\n",
    "# Create the CNNLSTM model\n",
    "model = CNNLSTM()\n",
    "print(model)\n",
    "\n",
    "# Example input\n",
    "x = torch.randn(1, 10, 1, 1500, 121)\n",
    "output = model(x)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTM(\n",
      "  (conv_lstm): ConvLSTM(\n",
      "    (cells): ModuleList(\n",
      "      (0): ConvLSTMCell(\n",
      "        (encoder): Sequential(\n",
      "          (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (conv): Conv2d(17, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): CNNDecoder(\n",
      "    (deconv1): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (deconv2): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 10, 1, 1500, 120])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ConvLSTM单元，并加入Encoder缩小图像尺寸\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, dropout=0.2):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "\n",
    "        # Encoder部分缩小图像尺寸\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, input_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(input_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 添加池化层\n",
    "        )\n",
    "        \n",
    "        # ConvLSTM部分\n",
    "        self.conv = nn.Conv2d(input_channels + hidden_channels, 4 * hidden_channels, kernel_size, padding=self.padding)\n",
    "        self.bn = nn.BatchNorm2d(4 * hidden_channels)\n",
    "\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_cur, c_cur = hidden\n",
    "\n",
    "        # 先通过Encoder缩小图像尺寸\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # 确保h_cur和x的尺寸一致\n",
    "        if h_cur.size()[2:] != x.size()[2:]:\n",
    "            h_cur = nn.functional.interpolate(h_cur, size=x.size()[2:], mode='bilinear', align_corners=False)\n",
    "            c_cur = nn.functional.interpolate(c_cur, size=x.size()[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # 将Encoder的输出与当前的隐藏状态拼接\n",
    "        combined = torch.cat([x, h_cur], dim=1)\n",
    "        \n",
    "        # 批量归一化\n",
    "        conv_output = self.bn(self.conv(combined))\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        # Dropout\n",
    "        h_next = self.dropout(h_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "\n",
    "# ConvLSTM层，调用ConvLSTMCell\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, num_layers):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.cells = nn.ModuleList([ConvLSTMCell(input_channels if i == 0 else hidden_channels,\n",
    "                                                 hidden_channels, kernel_size) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        hidden = [(torch.zeros(batch_size, self.hidden_channels, height // 2, width // 2).to(x.device),\n",
    "                   torch.zeros(batch_size, self.hidden_channels, height // 2, width // 2).to(x.device)) for _ in range(self.num_layers)]\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :, :, :]\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                hidden[i] = cell(x_t, hidden[i])\n",
    "                x_t = hidden[i][0]\n",
    "            outputs.append(x_t.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Decoder使用反卷积逐步恢复图像尺寸\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        \n",
    "        # 反卷积层，恢复图像尺寸\n",
    "        self.deconv1 = nn.ConvTranspose2d(input_channels, input_channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(input_channels // 2)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(input_channels // 2, output_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.deconv1(x)))\n",
    "        x = self.deconv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 总体的CNNLSTM模型，结合ConvLSTM与Decoder\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.conv_lstm = ConvLSTM(input_channels=1, hidden_channels=16, kernel_size=3, num_layers=1)\n",
    "        self.decoder = CNNDecoder(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out = self.conv_lstm(x)\n",
    "        batch_size, seq_len, channels, height, width = lstm_out.size()\n",
    "        decoded_frames = []\n",
    "        for t in range(seq_len):\n",
    "            decoded_frame = self.decoder(lstm_out[:, t, :, :, :])\n",
    "            decoded_frames.append(decoded_frame.unsqueeze(1))\n",
    "        decoded_frames = torch.cat(decoded_frames, dim=1)\n",
    "        return decoded_frames\n",
    "\n",
    "\n",
    "# 测试模型\n",
    "model = CNNLSTM()\n",
    "print(model)\n",
    "\n",
    "# 示例输入\n",
    "x = torch.randn(1, 10, 1, 1500, 121)\n",
    "output = model(x)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTM(\n",
      "  (conv_lstm): ConvLSTM(\n",
      "    (cells): ModuleList(\n",
      "      (0): ConvLSTMCell(\n",
      "        (encoder): Sequential(\n",
      "          (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (conv): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): ConvLSTMCell(\n",
      "        (encoder): Sequential(\n",
      "          (0): Conv2d(2, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (conv): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): CNNDecoder(\n",
      "    (deconv1): ConvTranspose2d(2, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (deconv2): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 10, 1, 1500, 121])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ConvLSTM单元，并加入Encoder缩小图像尺寸\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, dropout=0.2):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "\n",
    "        # Encoder部分缩小图像尺寸\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, input_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(input_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)  # 添加池化层\n",
    "        )\n",
    "        \n",
    "        # ConvLSTM部分\n",
    "        self.conv = nn.Conv2d(input_channels + hidden_channels, 4 * hidden_channels, kernel_size, padding=self.padding)\n",
    "        self.bn = nn.BatchNorm2d(4 * hidden_channels)\n",
    "\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_cur, c_cur = hidden\n",
    "\n",
    "        # 先通过Encoder缩小图像尺寸\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # 确保h_cur和x的尺寸一致\n",
    "        if h_cur.size()[2:] != x.size()[2:]:\n",
    "            h_cur = nn.functional.interpolate(h_cur, size=x.size()[2:], mode='bilinear', align_corners=False)\n",
    "            c_cur = nn.functional.interpolate(c_cur, size=x.size()[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # 将Encoder的输出与当前的隐藏状态拼接\n",
    "        combined = torch.cat([x, h_cur], dim=1)\n",
    "        \n",
    "        # 批量归一化\n",
    "        conv_output = self.bn(self.conv(combined))\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        # Dropout\n",
    "        h_next = self.dropout(h_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "# ConvLSTM层，调用ConvLSTMCell\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, num_layers):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.cells = nn.ModuleList([ConvLSTMCell(input_channels if i == 0 else hidden_channels,\n",
    "                                                 hidden_channels, kernel_size) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        hidden = [(torch.zeros(batch_size, self.hidden_channels, height // 2, width // 2).to(x.device),\n",
    "                   torch.zeros(batch_size, self.hidden_channels, height // 2, width // 2).to(x.device)) for _ in range(self.num_layers)]\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :, :, :]\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                hidden[i] = cell(x_t, hidden[i])\n",
    "                x_t = hidden[i][0]\n",
    "            outputs.append(x_t.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "    \n",
    "# Decoder使用反卷积逐步恢复图像尺寸\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        \n",
    "        # 反卷积层，恢复图像尺寸\n",
    "        self.deconv1 = nn.ConvTranspose2d(input_channels, input_channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(input_channels // 2)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(input_channels // 2, output_channels, kernel_size=3, stride=2, padding=0, output_padding=0)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.deconv1(x)))\n",
    "        x = self.deconv2(x)\n",
    "        return x\n",
    "\n",
    "# 总体的CNNLSTM模型，结合ConvLSTM与Decoder\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.conv_lstm = ConvLSTM(input_channels=1, hidden_channels=2, kernel_size=3, num_layers=2)\n",
    "        self.decoder = CNNDecoder(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        lstm_out = self.conv_lstm(x)\n",
    "        # _, seq_len, channels, _, _ = lstm_out.size()\n",
    "        decoded_frames = []\n",
    "        for t in range(seq_len):\n",
    "            decoded_frame = self.decoder(lstm_out[:, t, :, :, :])[:, :, :height, :width]\n",
    "            decoded_frames.append(decoded_frame.unsqueeze(1))\n",
    "        decoded_frames = torch.cat(decoded_frames, dim=1)\n",
    "        return decoded_frames\n",
    "\n",
    "\n",
    "# 测试模型\n",
    "model = CNNLSTM()\n",
    "print(model)\n",
    "\n",
    "# 示例输入\n",
    "x = torch.randn(1, 10, 1, 1500, 121)\n",
    "output = model(x)\n",
    "print(output.shape)  # 确保输出尺寸与输入匹配\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (convlstm1): ConvLSTM(\n",
      "    (cell): ConvLSTMCell(\n",
      "      (conv): Conv2d(65, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convlstm2): ConvLSTM(\n",
      "    (cell): ConvLSTMCell(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convlstm3): ConvLSTM(\n",
      "    (cell): ConvLSTMCell(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (conv3d): Conv3d(64, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "torch.Size([1, 10, 64, 1500, 121])\n",
      "torch.Size([1, 64, 10, 1500, 121])\n",
      "torch.Size([1, 10, 64, 1500, 121])\n",
      "torch.Size([1, 64, 10, 1500, 121])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 128, 1, 1], expected input[1, 74, 1500, 121] to have 128 channels, but got 74 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(input_channels)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m---> 92\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# 确保输出尺寸与输入匹配\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ljy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ljy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 76\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvlstm3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Prepare for Conv3D: permute to (batch_size, channels, seq_len, height, width)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ljy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ljy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 47\u001b[0m, in \u001b[0;36mConvLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[1;32m---> 47\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(hidden_state[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_sequences:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ljy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ljy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m, in \u001b[0;36mConvLSTMCell.forward\u001b[1;34m(self, x, hidden_state)\u001b[0m\n\u001b[0;32m     21\u001b[0m h, c \u001b[38;5;241m=\u001b[39m hidden_state\n\u001b[0;32m     22\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, h), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# concat along channel axis\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Split the output into four parts\u001b[39;00m\n\u001b[0;32m     26\u001b[0m i, f, o, g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(conv_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ljy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ljy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ljy\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ljy\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 128, 1, 1], expected input[1, 74, 1500, 121] to have 128 channels, but got 74 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_dim + hidden_dim,\n",
    "            out_channels=4 * hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=(kernel_size[0] // 2, kernel_size[1] // 2),\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        h, c = hidden_state\n",
    "        combined = torch.cat((x, h), dim=1)  # concat along channel axis\n",
    "        conv_out = self.conv(combined)\n",
    "        \n",
    "        # Split the output into four parts\n",
    "        i, f, o, g = torch.split(conv_out, self.hidden_dim, dim=1)\n",
    "\n",
    "        # Compute the new cell state and hidden state\n",
    "        new_c = (f * c) + (i * torch.tanh(g))\n",
    "        new_h = o * torch.tanh(new_c)\n",
    "\n",
    "        return new_h, new_c\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, return_sequences=True):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.cell = ConvLSTMCell(input_dim, hidden_dim, kernel_size)\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, C, H, W = x.size()\n",
    "        hidden_state = (torch.zeros(batch_size, self.cell.hidden_dim, H, W).to(x.device),\n",
    "                        torch.zeros(batch_size, self.cell.hidden_dim, H, W).to(x.device))\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            hidden_state = self.cell(x[:, t, :, :, :], hidden_state)\n",
    "            outputs.append(hidden_state[0])\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return torch.stack(outputs, dim=1)\n",
    "        else:\n",
    "            return outputs[-1]\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Model, self).__init__()\n",
    "        self.convlstm1 = ConvLSTM(input_channels, 64, kernel_size=(5, 5), return_sequences=True)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.convlstm2 = ConvLSTM(64, 64, kernel_size=(3, 3), return_sequences=True)\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.convlstm3 = ConvLSTM(64, 64, kernel_size=(1, 1), return_sequences=True)\n",
    "        \n",
    "        self.conv3d = nn.Conv3d(64, 1, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convlstm1(x)  # Input shape: (batch_size, seq_len, channels, height, width)\n",
    "        print(x.shape)\n",
    "        x = self.bn1(x.permute(0, 2, 1, 3, 4))  # Apply BatchNorm3d, permute to (batch_size, channels, seq_len, height, width)\n",
    "        print(x.shape)\n",
    "        x = self.convlstm2(x.permute(0, 2, 1, 3, 4))\n",
    "        print(x.shape)\n",
    "        x = self.bn2(x.permute(0, 2, 1, 3, 4))\n",
    "        print(x.shape)\n",
    "        x = self.convlstm3(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        # Prepare for Conv3D: permute to (batch_size, channels, seq_len, height, width)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        x = self.conv3d(x)  # Apply Conv3D\n",
    "        x = self.sigmoid(x)  # Sigmoid activation for binary classification\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# 示例输入\n",
    "x = torch.randn(1, 10, 1, 1500, 121)\n",
    "input_channels = x.shape[2]\n",
    "model = Model(input_channels)\n",
    "print(model)\n",
    "output = model(x)\n",
    "print(output.shape)  # 确保输出尺寸与输入匹配\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 加载训练所需脚本（训练过程中绘制图片）\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "output_dir = 'output_images'\n",
    "\n",
    "def img_process(image):\n",
    "    # image = image.transpose(1, 0)\n",
    "    # Convert numpy arrays to PIL Images\n",
    "    image = (image).astype(np.uint8)\n",
    "    pil_image = Image.fromarray(image)\n",
    "    return pil_image\n",
    "\n",
    "def save_img(batch_data, outputs, batch_labels=None, num=0, output_dir='output_images'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for j in range(batch_data.size(0)):\n",
    "        input_image = batch_data[j].detach().squeeze().cpu().numpy()\n",
    "        output_image = outputs[j].detach().squeeze().cpu().numpy()\n",
    "\n",
    "        for i_length in range(output_image.shape[0]):\n",
    "\n",
    "            input_pil = img_process(input_image[i_length])\n",
    "            output_pil = img_process(output_image[i_length])\n",
    "        \n",
    "            # Save images\n",
    "            input_pil.save(os.path.join(output_dir, f'input_{num * batch_data.size(0) + j}_{i_length}.png'))\n",
    "            output_pil.save(os.path.join(output_dir, f'output_{num * batch_data.size(0) + j}_{i_length}.png'))\n",
    "\n",
    "            if batch_labels!=None:\n",
    "                label_image = batch_labels[j].squeeze().cpu().numpy()\n",
    "                label_pil = img_process(label_image[i_length])\n",
    "                label_pil.save(os.path.join(output_dir, f'label_{num * batch_data.size(0) + j}_{i_length}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/50, batch: 0, Loss: 174.14556884765625, Time:0.6866855621337891\n",
      "epoch: 1/50, batch: 100, Loss: 162.29408264160156, Time:33.925121784210205\n",
      "epoch: 1/50, batch: 200, Loss: 143.16339111328125, Time:33.8348331451416\n",
      "epoch: 1/50, batch: 300, Loss: 133.12728881835938, Time:34.46124863624573\n",
      "epoch: 1/50, batch: 400, Loss: 127.62744140625, Time:34.74499726295471\n",
      "epoch: 1/50, batch: 500, Loss: 125.11872863769531, Time:34.67565608024597\n",
      "epoch: 1/50, batch: 600, Loss: 120.153076171875, Time:34.35886764526367\n",
      "epoch: 1/50, batch: 700, Loss: 119.00977325439453, Time:33.85159945487976\n",
      "epoch: 1/50, batch: 800, Loss: 117.60094451904297, Time:33.87755274772644\n",
      "epoch: 1/50, batch: 900, Loss: 116.74979400634766, Time:33.851019859313965\n",
      "epoch: 1/50, batch: 1000, Loss: 115.39795684814453, Time:33.8439404964447\n",
      "epoch: 1/50, batch: 1100, Loss: 113.65389251708984, Time:33.827295541763306\n",
      "epoch: 1/50, batch: 1200, Loss: 115.65608978271484, Time:34.6987669467926\n",
      "epoch: 1/50, batch: 1300, Loss: 115.8050765991211, Time:33.96087956428528\n",
      "epoch: 1/50, batch: 1400, Loss: 112.6657485961914, Time:33.81073975563049\n",
      "epoch: 1/50, batch: 1500, Loss: 113.36711883544922, Time:33.94425821304321\n",
      "epoch: 1/50, batch: 1600, Loss: 117.68450164794922, Time:33.844178915023804\n",
      "epoch: 1/50, batch: 1700, Loss: 112.21745300292969, Time:34.12794351577759\n",
      "Epoch 1/50, Loss: 124.44470507777523\n",
      "Val Loss: 115.51202289814852\n",
      "epoch: 2/50, batch: 0, Loss: 108.19550323486328, Time:41.231167793273926\n",
      "epoch: 2/50, batch: 100, Loss: 115.0229721069336, Time:33.644060134887695\n",
      "epoch: 2/50, batch: 200, Loss: 112.33927154541016, Time:33.67732882499695\n",
      "epoch: 2/50, batch: 300, Loss: 110.54723358154297, Time:33.726388931274414\n",
      "epoch: 2/50, batch: 400, Loss: 112.33951568603516, Time:33.71238994598389\n",
      "epoch: 2/50, batch: 500, Loss: 111.13914489746094, Time:33.66258430480957\n",
      "epoch: 2/50, batch: 600, Loss: 114.08329772949219, Time:33.677290201187134\n",
      "epoch: 2/50, batch: 700, Loss: 116.0408706665039, Time:33.677314043045044\n",
      "epoch: 2/50, batch: 800, Loss: 111.00948333740234, Time:33.94430613517761\n",
      "epoch: 2/50, batch: 900, Loss: 117.16913604736328, Time:34.04409384727478\n",
      "epoch: 2/50, batch: 1000, Loss: 113.95341491699219, Time:33.877243757247925\n",
      "epoch: 2/50, batch: 1100, Loss: 114.46293640136719, Time:33.8441698551178\n",
      "epoch: 2/50, batch: 1200, Loss: 110.1047134399414, Time:33.84422945976257\n",
      "epoch: 2/50, batch: 1300, Loss: 111.11329650878906, Time:33.86086630821228\n",
      "epoch: 2/50, batch: 1400, Loss: 113.69315338134766, Time:33.860655546188354\n",
      "epoch: 2/50, batch: 1500, Loss: 112.09825134277344, Time:33.844109535217285\n",
      "epoch: 2/50, batch: 1600, Loss: 115.25208282470703, Time:33.86049938201904\n",
      "epoch: 2/50, batch: 1700, Loss: 108.85505676269531, Time:33.84429049491882\n",
      "Epoch 2/50, Loss: 113.7528730900718\n",
      "Val Loss: 115.63514145831672\n",
      "epoch: 3/50, batch: 0, Loss: 117.03640747070312, Time:44.06882119178772\n",
      "epoch: 3/50, batch: 100, Loss: 112.78192138671875, Time:33.87757682800293\n",
      "epoch: 3/50, batch: 200, Loss: 114.04146575927734, Time:33.8608021736145\n",
      "epoch: 3/50, batch: 300, Loss: 111.25663757324219, Time:34.091498613357544\n",
      "epoch: 3/50, batch: 400, Loss: 115.25760650634766, Time:33.6925995349884\n",
      "epoch: 3/50, batch: 500, Loss: 114.75924682617188, Time:33.56055808067322\n",
      "epoch: 3/50, batch: 600, Loss: 111.81741333007812, Time:33.99330925941467\n",
      "epoch: 3/50, batch: 700, Loss: 115.49311828613281, Time:33.94414567947388\n",
      "epoch: 3/50, batch: 800, Loss: 111.61515808105469, Time:33.610297203063965\n",
      "epoch: 3/50, batch: 900, Loss: 109.45883178710938, Time:33.9134521484375\n",
      "epoch: 3/50, batch: 1000, Loss: 114.50430297851562, Time:34.05704998970032\n",
      "epoch: 3/50, batch: 1100, Loss: 101.24520874023438, Time:33.64627504348755\n",
      "epoch: 3/50, batch: 1200, Loss: 111.44503784179688, Time:33.59382677078247\n",
      "epoch: 3/50, batch: 1300, Loss: 116.67113494873047, Time:33.99385666847229\n",
      "epoch: 3/50, batch: 1400, Loss: 111.57939147949219, Time:33.808828592300415\n",
      "epoch: 3/50, batch: 1500, Loss: 111.90145874023438, Time:33.65032482147217\n",
      "epoch: 3/50, batch: 1600, Loss: 115.910888671875, Time:33.70317006111145\n",
      "epoch: 3/50, batch: 1700, Loss: 113.970947265625, Time:33.67963099479675\n",
      "Epoch 3/50, Loss: 112.44578945302007\n",
      "Val Loss: 115.40822813072982\n",
      "epoch: 4/50, batch: 0, Loss: 102.08906555175781, Time:41.33196210861206\n",
      "epoch: 4/50, batch: 100, Loss: 111.73218536376953, Time:33.69334936141968\n",
      "epoch: 4/50, batch: 200, Loss: 107.1208724975586, Time:33.728397607803345\n",
      "epoch: 4/50, batch: 300, Loss: 111.57569122314453, Time:33.64398956298828\n",
      "epoch: 4/50, batch: 400, Loss: 112.56845092773438, Time:33.70990967750549\n",
      "epoch: 4/50, batch: 500, Loss: 111.03279113769531, Time:33.679879903793335\n",
      "epoch: 4/50, batch: 600, Loss: 112.27021789550781, Time:33.81841278076172\n",
      "epoch: 4/50, batch: 700, Loss: 108.85079193115234, Time:33.54150104522705\n",
      "epoch: 4/50, batch: 800, Loss: 111.71310424804688, Time:33.51121950149536\n",
      "epoch: 4/50, batch: 900, Loss: 111.45146179199219, Time:33.49377751350403\n",
      "epoch: 4/50, batch: 1000, Loss: 113.59905242919922, Time:33.50795555114746\n",
      "epoch: 4/50, batch: 1100, Loss: 116.54714965820312, Time:33.52935743331909\n",
      "epoch: 4/50, batch: 1200, Loss: 110.62313842773438, Time:33.5104775428772\n",
      "epoch: 4/50, batch: 1300, Loss: 112.47305297851562, Time:33.510523080825806\n",
      "epoch: 4/50, batch: 1400, Loss: 113.11097717285156, Time:33.51044821739197\n",
      "epoch: 4/50, batch: 1500, Loss: 115.23944854736328, Time:33.49609446525574\n",
      "epoch: 4/50, batch: 1600, Loss: 106.82621002197266, Time:33.51051688194275\n",
      "epoch: 4/50, batch: 1700, Loss: 111.64437866210938, Time:33.50746726989746\n",
      "Epoch 4/50, Loss: 111.77088139924757\n",
      "Val Loss: 114.66708626260562\n",
      "epoch: 5/50, batch: 0, Loss: 110.83541870117188, Time:41.081541538238525\n",
      "epoch: 5/50, batch: 100, Loss: 108.02291107177734, Time:33.493828773498535\n",
      "epoch: 5/50, batch: 200, Loss: 112.34061431884766, Time:33.52721929550171\n",
      "epoch: 5/50, batch: 300, Loss: 110.3387680053711, Time:33.508625507354736\n",
      "epoch: 5/50, batch: 400, Loss: 113.49968719482422, Time:33.59070014953613\n",
      "epoch: 5/50, batch: 500, Loss: 111.8504867553711, Time:34.328887701034546\n",
      "epoch: 5/50, batch: 600, Loss: 112.35855865478516, Time:33.561601638793945\n",
      "epoch: 5/50, batch: 700, Loss: 113.5336685180664, Time:33.610541105270386\n",
      "epoch: 5/50, batch: 800, Loss: 109.40447235107422, Time:33.66329503059387\n",
      "epoch: 5/50, batch: 900, Loss: 109.32986450195312, Time:33.64584231376648\n",
      "epoch: 5/50, batch: 1000, Loss: 108.3787841796875, Time:33.75920367240906\n",
      "epoch: 5/50, batch: 1100, Loss: 107.4405746459961, Time:33.59363651275635\n",
      "epoch: 5/50, batch: 1200, Loss: 105.92715454101562, Time:33.52858567237854\n",
      "epoch: 5/50, batch: 1300, Loss: 114.67374420166016, Time:33.62884783744812\n",
      "epoch: 5/50, batch: 1400, Loss: 108.16229248046875, Time:33.6106436252594\n",
      "epoch: 5/50, batch: 1500, Loss: 111.44552612304688, Time:33.746264696121216\n",
      "epoch: 5/50, batch: 1600, Loss: 110.60694122314453, Time:34.18766760826111\n",
      "epoch: 5/50, batch: 1700, Loss: 112.39573669433594, Time:34.208943367004395\n",
      "Epoch 5/50, Loss: 111.21665391457456\n",
      "Val Loss: 115.98158647186902\n",
      "epoch: 6/50, batch: 0, Loss: 113.61048889160156, Time:44.125208616256714\n",
      "epoch: 6/50, batch: 100, Loss: 112.6358871459961, Time:34.02356553077698\n",
      "epoch: 6/50, batch: 200, Loss: 108.17782592773438, Time:34.01095652580261\n",
      "epoch: 6/50, batch: 300, Loss: 115.79135131835938, Time:33.96098065376282\n",
      "epoch: 6/50, batch: 400, Loss: 111.4103775024414, Time:33.92749643325806\n",
      "epoch: 6/50, batch: 500, Loss: 110.01612854003906, Time:34.02955174446106\n",
      "epoch: 6/50, batch: 600, Loss: 105.17826080322266, Time:34.4117157459259\n",
      "epoch: 6/50, batch: 700, Loss: 113.56013488769531, Time:33.6792197227478\n",
      "epoch: 6/50, batch: 800, Loss: 108.53007507324219, Time:33.610982179641724\n",
      "epoch: 6/50, batch: 900, Loss: 108.72566986083984, Time:33.52963709831238\n",
      "epoch: 6/50, batch: 1000, Loss: 111.57335662841797, Time:33.8906729221344\n",
      "epoch: 6/50, batch: 1100, Loss: 113.22015380859375, Time:33.813347578048706\n",
      "epoch: 6/50, batch: 1200, Loss: 110.5926513671875, Time:33.591657638549805\n",
      "epoch: 6/50, batch: 1300, Loss: 107.20530700683594, Time:33.51707100868225\n",
      "epoch: 6/50, batch: 1400, Loss: 108.43646240234375, Time:33.91637873649597\n",
      "epoch: 6/50, batch: 1500, Loss: 114.07390594482422, Time:33.58419370651245\n",
      "epoch: 6/50, batch: 1600, Loss: 111.55038452148438, Time:33.19505000114441\n",
      "epoch: 6/50, batch: 1700, Loss: 109.99757385253906, Time:33.95475959777832\n",
      "Epoch 6/50, Loss: 110.81088716348468\n",
      "Val Loss: 116.58842748525191\n",
      "epoch: 7/50, batch: 0, Loss: 111.36148071289062, Time:41.6673002243042\n",
      "epoch: 7/50, batch: 100, Loss: 111.2286605834961, Time:33.960991621017456\n",
      "epoch: 7/50, batch: 200, Loss: 110.91411590576172, Time:34.04810333251953\n",
      "epoch: 7/50, batch: 300, Loss: 113.44335174560547, Time:34.544697523117065\n",
      "epoch: 7/50, batch: 400, Loss: 111.48961639404297, Time:34.294445753097534\n",
      "epoch: 7/50, batch: 500, Loss: 113.11072540283203, Time:34.332722187042236\n",
      "epoch: 7/50, batch: 600, Loss: 109.10792541503906, Time:33.72737383842468\n",
      "epoch: 7/50, batch: 700, Loss: 108.2186050415039, Time:33.72728681564331\n",
      "epoch: 7/50, batch: 800, Loss: 112.49798583984375, Time:33.77736973762512\n",
      "epoch: 7/50, batch: 900, Loss: 111.44415283203125, Time:33.73522591590881\n",
      "epoch: 7/50, batch: 1000, Loss: 108.6054916381836, Time:33.71249318122864\n",
      "epoch: 7/50, batch: 1100, Loss: 113.41937255859375, Time:33.7606885433197\n",
      "epoch: 7/50, batch: 1200, Loss: 112.0013427734375, Time:33.76071619987488\n",
      "epoch: 7/50, batch: 1300, Loss: 114.83552551269531, Time:33.760737895965576\n",
      "epoch: 7/50, batch: 1400, Loss: 106.20331573486328, Time:33.74401021003723\n",
      "epoch: 7/50, batch: 1500, Loss: 109.53730010986328, Time:33.728280544281006\n",
      "epoch: 7/50, batch: 1600, Loss: 108.3237533569336, Time:33.76072335243225\n",
      "epoch: 7/50, batch: 1700, Loss: 109.16454315185547, Time:33.743950843811035\n",
      "Epoch 7/50, Loss: 110.43875166228986\n",
      "Val Loss: 118.03548119992627\n",
      "epoch: 8/50, batch: 0, Loss: 109.50530242919922, Time:41.35019016265869\n",
      "epoch: 8/50, batch: 100, Loss: 108.93802642822266, Time:33.76077342033386\n",
      "epoch: 8/50, batch: 200, Loss: 109.79749298095703, Time:33.7439968585968\n",
      "epoch: 8/50, batch: 300, Loss: 111.67878723144531, Time:33.74401783943176\n",
      "epoch: 8/50, batch: 400, Loss: 114.99635314941406, Time:33.760703325271606\n",
      "epoch: 8/50, batch: 500, Loss: 111.05992126464844, Time:33.744038105010986\n",
      "epoch: 8/50, batch: 600, Loss: 108.61259460449219, Time:33.744136571884155\n",
      "epoch: 8/50, batch: 700, Loss: 110.23809051513672, Time:33.97012972831726\n",
      "epoch: 8/50, batch: 800, Loss: 112.58114624023438, Time:34.23581552505493\n",
      "epoch: 8/50, batch: 900, Loss: 112.08453369140625, Time:33.92195439338684\n",
      "epoch: 8/50, batch: 1000, Loss: 115.0546646118164, Time:33.758986949920654\n",
      "epoch: 8/50, batch: 1100, Loss: 112.62263488769531, Time:33.77497410774231\n",
      "epoch: 8/50, batch: 1200, Loss: 109.5924301147461, Time:33.68180251121521\n",
      "epoch: 8/50, batch: 1300, Loss: 110.79076385498047, Time:33.874367237091064\n",
      "epoch: 8/50, batch: 1400, Loss: 108.88648986816406, Time:34.87763476371765\n",
      "epoch: 8/50, batch: 1500, Loss: 112.99091339111328, Time:34.18844938278198\n",
      "epoch: 8/50, batch: 1600, Loss: 107.47709655761719, Time:34.242053270339966\n",
      "epoch: 8/50, batch: 1700, Loss: 113.1795654296875, Time:34.304150104522705\n",
      "Epoch 8/50, Loss: 110.15319390064666\n",
      "Val Loss: 118.13168026671117\n",
      "epoch: 9/50, batch: 0, Loss: 110.98902893066406, Time:44.76214599609375\n",
      "epoch: 9/50, batch: 100, Loss: 114.56351470947266, Time:34.26000952720642\n",
      "epoch: 9/50, batch: 200, Loss: 109.2611312866211, Time:34.259316205978394\n",
      "epoch: 9/50, batch: 300, Loss: 110.2332992553711, Time:34.3686625957489\n",
      "epoch: 9/50, batch: 400, Loss: 111.99299621582031, Time:34.32930898666382\n",
      "epoch: 9/50, batch: 500, Loss: 107.38958740234375, Time:34.26315903663635\n",
      "epoch: 9/50, batch: 600, Loss: 108.49388885498047, Time:34.304877519607544\n",
      "epoch: 9/50, batch: 700, Loss: 108.40998077392578, Time:34.42949414253235\n",
      "epoch: 9/50, batch: 800, Loss: 109.52549743652344, Time:34.263166666030884\n",
      "epoch: 9/50, batch: 900, Loss: 111.44656372070312, Time:34.29442596435547\n",
      "epoch: 9/50, batch: 1000, Loss: 112.5473403930664, Time:34.61144399642944\n",
      "epoch: 9/50, batch: 1100, Loss: 109.85040283203125, Time:34.47790575027466\n",
      "epoch: 9/50, batch: 1200, Loss: 114.82205200195312, Time:34.59463882446289\n",
      "epoch: 9/50, batch: 1300, Loss: 108.9906234741211, Time:34.628273487091064\n",
      "epoch: 9/50, batch: 1400, Loss: 113.75092315673828, Time:34.26406526565552\n",
      "epoch: 9/50, batch: 1500, Loss: 108.24958801269531, Time:34.17915439605713\n",
      "epoch: 9/50, batch: 1600, Loss: 106.05815124511719, Time:34.317373514175415\n",
      "epoch: 9/50, batch: 1700, Loss: 108.80299377441406, Time:34.62788152694702\n",
      "Epoch 9/50, Loss: 109.8487874465549\n",
      "Val Loss: 117.93975867446588\n",
      "epoch: 10/50, batch: 0, Loss: 113.68937683105469, Time:45.052170276641846\n",
      "epoch: 10/50, batch: 100, Loss: 108.38156127929688, Time:33.93097972869873\n",
      "epoch: 10/50, batch: 200, Loss: 109.6955795288086, Time:34.103105783462524\n",
      "epoch: 10/50, batch: 300, Loss: 107.50921630859375, Time:34.13274264335632\n",
      "epoch: 10/50, batch: 400, Loss: 109.14910125732422, Time:33.94676208496094\n",
      "epoch: 10/50, batch: 500, Loss: 113.18328094482422, Time:33.96228551864624\n",
      "epoch: 10/50, batch: 600, Loss: 104.43328094482422, Time:33.94740056991577\n",
      "epoch: 10/50, batch: 700, Loss: 108.99945068359375, Time:33.9317569732666\n",
      "epoch: 10/50, batch: 800, Loss: 106.1967544555664, Time:33.9796724319458\n",
      "epoch: 10/50, batch: 900, Loss: 112.1075439453125, Time:33.99205470085144\n",
      "epoch: 10/50, batch: 1000, Loss: 109.38111877441406, Time:33.993284463882446\n",
      "epoch: 10/50, batch: 1100, Loss: 113.27241516113281, Time:33.976064920425415\n",
      "epoch: 10/50, batch: 1200, Loss: 110.22370147705078, Time:34.008920431137085\n",
      "epoch: 10/50, batch: 1300, Loss: 112.57186126708984, Time:33.97919201850891\n",
      "epoch: 10/50, batch: 1400, Loss: 107.55796813964844, Time:33.97795748710632\n",
      "epoch: 10/50, batch: 1500, Loss: 130.95669555664062, Time:33.968674421310425\n",
      "epoch: 10/50, batch: 1600, Loss: 107.17987060546875, Time:33.93083691596985\n",
      "epoch: 10/50, batch: 1700, Loss: 112.40669250488281, Time:33.97821116447449\n",
      "Epoch 10/50, Loss: 109.67396763580234\n",
      "Val Loss: 120.15247709313218\n",
      "epoch: 11/50, batch: 0, Loss: 109.78756713867188, Time:44.246878147125244\n",
      "epoch: 11/50, batch: 100, Loss: 106.58846282958984, Time:34.00183916091919\n",
      "epoch: 11/50, batch: 200, Loss: 105.585205078125, Time:34.04118227958679\n",
      "epoch: 11/50, batch: 300, Loss: 114.94964599609375, Time:33.933212757110596\n",
      "epoch: 11/50, batch: 400, Loss: 108.89196014404297, Time:33.978671073913574\n",
      "epoch: 11/50, batch: 500, Loss: 110.45626831054688, Time:33.98329544067383\n",
      "epoch: 11/50, batch: 600, Loss: 109.41276550292969, Time:33.94632363319397\n",
      "epoch: 11/50, batch: 700, Loss: 108.04631805419922, Time:34.21660804748535\n",
      "epoch: 11/50, batch: 800, Loss: 112.71065521240234, Time:33.87746214866638\n",
      "epoch: 11/50, batch: 900, Loss: 110.57249450683594, Time:33.8941113948822\n",
      "epoch: 11/50, batch: 1000, Loss: 111.37847137451172, Time:33.927549839019775\n",
      "epoch: 11/50, batch: 1100, Loss: 107.86935424804688, Time:33.877612829208374\n",
      "epoch: 11/50, batch: 1200, Loss: 109.17811584472656, Time:33.87758827209473\n",
      "epoch: 11/50, batch: 1300, Loss: 108.4587631225586, Time:33.87747097015381\n",
      "epoch: 11/50, batch: 1400, Loss: 110.7386474609375, Time:33.87755298614502\n",
      "epoch: 11/50, batch: 1500, Loss: 111.73121643066406, Time:33.86071014404297\n",
      "epoch: 11/50, batch: 1600, Loss: 109.98307800292969, Time:33.92762494087219\n",
      "epoch: 11/50, batch: 1700, Loss: 107.50677490234375, Time:33.927510023117065\n",
      "Epoch 11/50, Loss: 109.5492091225348\n",
      "Val Loss: 118.63521681805047\n",
      "epoch: 12/50, batch: 0, Loss: 109.91040802001953, Time:44.102598667144775\n",
      "epoch: 12/50, batch: 100, Loss: 111.74977111816406, Time:33.89417219161987\n",
      "epoch: 12/50, batch: 200, Loss: 107.50325775146484, Time:33.877530574798584\n",
      "epoch: 12/50, batch: 300, Loss: 105.55879974365234, Time:33.89522433280945\n",
      "epoch: 12/50, batch: 400, Loss: 106.65144348144531, Time:33.91186475753784\n",
      "epoch: 12/50, batch: 500, Loss: 105.09785461425781, Time:33.87747025489807\n",
      "epoch: 12/50, batch: 600, Loss: 108.74201202392578, Time:33.8942084312439\n",
      "epoch: 12/50, batch: 700, Loss: 110.531982421875, Time:33.8941605091095\n",
      "epoch: 12/50, batch: 800, Loss: 108.19923400878906, Time:33.9932119846344\n",
      "epoch: 12/50, batch: 900, Loss: 111.93732452392578, Time:33.894097328186035\n",
      "epoch: 12/50, batch: 1000, Loss: 110.45938110351562, Time:33.96089482307434\n",
      "epoch: 12/50, batch: 1100, Loss: 114.40571594238281, Time:33.96619749069214\n",
      "epoch: 12/50, batch: 1200, Loss: 108.07445526123047, Time:34.15808415412903\n",
      "epoch: 12/50, batch: 1300, Loss: 106.06338500976562, Time:34.71412467956543\n",
      "epoch: 12/50, batch: 1400, Loss: 109.58927154541016, Time:34.36615204811096\n",
      "epoch: 12/50, batch: 1500, Loss: 108.13792419433594, Time:34.747485399246216\n",
      "epoch: 12/50, batch: 1600, Loss: 107.22763061523438, Time:34.24760961532593\n",
      "epoch: 12/50, batch: 1700, Loss: 110.59224700927734, Time:34.58766222000122\n",
      "Epoch 12/50, Loss: 109.3933437883\n",
      "Val Loss: 118.87679026078204\n",
      "epoch: 13/50, batch: 0, Loss: 112.49425506591797, Time:44.21452736854553\n",
      "epoch: 13/50, batch: 100, Loss: 112.27661895751953, Time:34.011017084121704\n",
      "epoch: 13/50, batch: 200, Loss: 107.70729064941406, Time:34.6057915687561\n",
      "epoch: 13/50, batch: 300, Loss: 111.00272369384766, Time:34.03458619117737\n",
      "epoch: 13/50, batch: 400, Loss: 110.57616424560547, Time:33.891319036483765\n",
      "epoch: 13/50, batch: 500, Loss: 108.8014907836914, Time:34.03794527053833\n",
      "epoch: 13/50, batch: 600, Loss: 108.09333801269531, Time:33.938252210617065\n",
      "epoch: 13/50, batch: 700, Loss: 107.31768035888672, Time:34.06766867637634\n",
      "epoch: 13/50, batch: 800, Loss: 108.52164459228516, Time:33.911022424697876\n",
      "epoch: 13/50, batch: 900, Loss: 113.02114868164062, Time:34.37711572647095\n",
      "epoch: 13/50, batch: 1000, Loss: 108.6926040649414, Time:34.31947326660156\n",
      "epoch: 13/50, batch: 1100, Loss: 108.05448913574219, Time:34.518250703811646\n",
      "epoch: 13/50, batch: 1200, Loss: 109.0189437866211, Time:34.35519051551819\n",
      "epoch: 13/50, batch: 1300, Loss: 114.03173065185547, Time:33.81693959236145\n",
      "epoch: 13/50, batch: 1400, Loss: 113.19851684570312, Time:33.808759689331055\n",
      "epoch: 13/50, batch: 1500, Loss: 102.31668853759766, Time:33.84453511238098\n",
      "epoch: 13/50, batch: 1600, Loss: 112.1135482788086, Time:33.824542760849\n",
      "epoch: 13/50, batch: 1700, Loss: 111.90823364257812, Time:33.85628056526184\n",
      "Epoch 13/50, Loss: 109.23618314464318\n",
      "Val Loss: 120.86916310835858\n",
      "epoch: 14/50, batch: 0, Loss: 112.03398132324219, Time:45.176846504211426\n",
      "epoch: 14/50, batch: 100, Loss: 111.71914672851562, Time:34.02051115036011\n",
      "epoch: 14/50, batch: 200, Loss: 108.68511199951172, Time:33.8853178024292\n",
      "epoch: 14/50, batch: 300, Loss: 111.73075103759766, Time:33.864189863204956\n",
      "epoch: 14/50, batch: 400, Loss: 111.05451202392578, Time:34.6171510219574\n",
      "epoch: 14/50, batch: 500, Loss: 109.33599090576172, Time:34.503012895584106\n",
      "epoch: 14/50, batch: 600, Loss: 107.90260314941406, Time:34.61279273033142\n",
      "epoch: 14/50, batch: 700, Loss: 110.41285705566406, Time:34.51648020744324\n",
      "epoch: 14/50, batch: 800, Loss: 112.71631622314453, Time:34.292078733444214\n",
      "epoch: 14/50, batch: 900, Loss: 108.23931121826172, Time:34.22500658035278\n",
      "epoch: 14/50, batch: 1000, Loss: 110.91586303710938, Time:34.217185258865356\n",
      "epoch: 14/50, batch: 1100, Loss: 110.02205657958984, Time:34.2261598110199\n",
      "epoch: 14/50, batch: 1200, Loss: 112.07794952392578, Time:34.128499269485474\n",
      "epoch: 14/50, batch: 1300, Loss: 108.43196868896484, Time:34.4520902633667\n",
      "epoch: 14/50, batch: 1400, Loss: 105.74298095703125, Time:34.382091760635376\n",
      "epoch: 14/50, batch: 1500, Loss: 109.14855194091797, Time:34.505587100982666\n",
      "epoch: 14/50, batch: 1600, Loss: 109.40557098388672, Time:34.23091673851013\n",
      "epoch: 14/50, batch: 1700, Loss: 110.72779846191406, Time:34.235801219940186\n",
      "Epoch 14/50, Loss: 109.09797855584874\n",
      "Val Loss: 118.91824315908003\n",
      "epoch: 15/50, batch: 0, Loss: 108.15956115722656, Time:41.79884171485901\n",
      "epoch: 15/50, batch: 100, Loss: 105.59893035888672, Time:34.021313428878784\n",
      "epoch: 15/50, batch: 200, Loss: 110.61761474609375, Time:33.734539270401\n",
      "epoch: 15/50, batch: 300, Loss: 108.51567840576172, Time:33.74541211128235\n",
      "epoch: 15/50, batch: 400, Loss: 107.12928009033203, Time:34.525113344192505\n",
      "epoch: 15/50, batch: 500, Loss: 108.92987823486328, Time:34.12909960746765\n",
      "epoch: 15/50, batch: 600, Loss: 110.55484008789062, Time:34.112796783447266\n",
      "epoch: 15/50, batch: 700, Loss: 107.47244262695312, Time:33.89499020576477\n",
      "epoch: 15/50, batch: 800, Loss: 109.5652084350586, Time:34.47527551651001\n",
      "epoch: 15/50, batch: 900, Loss: 107.6606216430664, Time:34.416996002197266\n",
      "epoch: 15/50, batch: 1000, Loss: 108.8621597290039, Time:34.11129641532898\n",
      "epoch: 15/50, batch: 1100, Loss: 110.3636703491211, Time:33.907705783843994\n",
      "epoch: 15/50, batch: 1200, Loss: 112.57711029052734, Time:33.89255428314209\n",
      "epoch: 15/50, batch: 1300, Loss: 111.29304504394531, Time:34.463067293167114\n",
      "epoch: 15/50, batch: 1400, Loss: 107.75968933105469, Time:33.93657946586609\n",
      "epoch: 15/50, batch: 1500, Loss: 112.5554428100586, Time:34.259305238723755\n",
      "epoch: 15/50, batch: 1600, Loss: 111.95677185058594, Time:33.84173083305359\n",
      "epoch: 15/50, batch: 1700, Loss: 108.79977416992188, Time:33.93113470077515\n",
      "Epoch 15/50, Loss: 109.02595172433935\n",
      "Val Loss: 120.83425604372609\n",
      "epoch: 16/50, batch: 0, Loss: 105.67900085449219, Time:43.676859617233276\n",
      "epoch: 16/50, batch: 100, Loss: 110.18881225585938, Time:33.89514708518982\n",
      "epoch: 16/50, batch: 200, Loss: 108.97844696044922, Time:33.8863890171051\n",
      "epoch: 16/50, batch: 300, Loss: 107.05056762695312, Time:33.8493173122406\n",
      "epoch: 16/50, batch: 400, Loss: 108.43980407714844, Time:33.590540647506714\n",
      "epoch: 16/50, batch: 500, Loss: 110.81351470947266, Time:33.90005874633789\n",
      "epoch: 16/50, batch: 600, Loss: 106.67515563964844, Time:33.883044958114624\n",
      "epoch: 16/50, batch: 700, Loss: 110.56121826171875, Time:33.94309663772583\n",
      "epoch: 16/50, batch: 800, Loss: 105.53972625732422, Time:33.85959529876709\n",
      "epoch: 16/50, batch: 900, Loss: 107.32229614257812, Time:33.906301975250244\n",
      "epoch: 16/50, batch: 1000, Loss: 107.54459381103516, Time:33.860352993011475\n",
      "epoch: 16/50, batch: 1100, Loss: 108.93843078613281, Time:33.56916785240173\n",
      "epoch: 16/50, batch: 1200, Loss: 112.35398864746094, Time:33.92350435256958\n",
      "epoch: 16/50, batch: 1300, Loss: 108.72313690185547, Time:33.92242622375488\n",
      "epoch: 16/50, batch: 1400, Loss: 109.5821304321289, Time:33.884591579437256\n",
      "epoch: 16/50, batch: 1500, Loss: 108.6636962890625, Time:33.60597896575928\n",
      "epoch: 16/50, batch: 1600, Loss: 114.62132263183594, Time:33.89346432685852\n",
      "epoch: 16/50, batch: 1700, Loss: 108.20206451416016, Time:33.57485342025757\n",
      "Epoch 16/50, Loss: 108.8725343327126\n",
      "Val Loss: 121.34289401307397\n",
      "epoch: 17/50, batch: 0, Loss: 109.74397277832031, Time:44.08606743812561\n",
      "epoch: 17/50, batch: 100, Loss: 110.26640319824219, Time:33.84138774871826\n",
      "epoch: 17/50, batch: 200, Loss: 111.73526763916016, Time:33.857056856155396\n",
      "epoch: 17/50, batch: 300, Loss: 107.74116516113281, Time:33.85943341255188\n",
      "epoch: 17/50, batch: 400, Loss: 109.52523040771484, Time:33.78920555114746\n",
      "epoch: 17/50, batch: 500, Loss: 112.25127410888672, Time:33.99620985984802\n",
      "epoch: 17/50, batch: 600, Loss: 107.26111602783203, Time:33.867021322250366\n",
      "epoch: 17/50, batch: 700, Loss: 101.78002166748047, Time:33.878132343292236\n",
      "epoch: 17/50, batch: 800, Loss: 107.60435485839844, Time:33.87217617034912\n",
      "epoch: 17/50, batch: 900, Loss: 109.25447082519531, Time:33.8509087562561\n",
      "epoch: 17/50, batch: 1000, Loss: 110.77330017089844, Time:33.68114686012268\n",
      "epoch: 17/50, batch: 1100, Loss: 109.46124267578125, Time:33.8939414024353\n",
      "epoch: 17/50, batch: 1200, Loss: 108.9566650390625, Time:33.90926384925842\n",
      "epoch: 17/50, batch: 1300, Loss: 106.88274383544922, Time:33.56079578399658\n",
      "epoch: 17/50, batch: 1400, Loss: 110.43220520019531, Time:33.90133476257324\n",
      "epoch: 17/50, batch: 1500, Loss: 110.97181701660156, Time:33.910510301589966\n",
      "epoch: 17/50, batch: 1600, Loss: 110.08358001708984, Time:33.88866424560547\n",
      "epoch: 17/50, batch: 1700, Loss: 109.4349365234375, Time:33.92891025543213\n",
      "Epoch 17/50, Loss: 108.78177020665909\n",
      "Val Loss: 121.56396082664023\n",
      "epoch: 18/50, batch: 0, Loss: 107.05186462402344, Time:42.926145792007446\n",
      "epoch: 18/50, batch: 100, Loss: 109.7735595703125, Time:33.87668228149414\n",
      "epoch: 18/50, batch: 200, Loss: 108.10127258300781, Time:33.921807289123535\n",
      "epoch: 18/50, batch: 300, Loss: 110.41255187988281, Time:33.92576479911804\n",
      "epoch: 18/50, batch: 400, Loss: 108.38062286376953, Time:33.6206328868866\n",
      "epoch: 18/50, batch: 500, Loss: 108.54534149169922, Time:33.88865113258362\n",
      "epoch: 18/50, batch: 600, Loss: 106.76931762695312, Time:33.559163331985474\n",
      "epoch: 18/50, batch: 700, Loss: 107.16770935058594, Time:34.07603073120117\n",
      "epoch: 18/50, batch: 800, Loss: 106.98613739013672, Time:33.47464990615845\n",
      "epoch: 18/50, batch: 900, Loss: 111.83195495605469, Time:33.49513077735901\n",
      "epoch: 18/50, batch: 1000, Loss: 107.64649963378906, Time:33.480271100997925\n",
      "epoch: 18/50, batch: 1100, Loss: 111.98739624023438, Time:33.50553369522095\n",
      "epoch: 18/50, batch: 1200, Loss: 106.52497100830078, Time:33.4703106880188\n",
      "epoch: 18/50, batch: 1300, Loss: 108.2256088256836, Time:33.49608540534973\n",
      "epoch: 18/50, batch: 1400, Loss: 108.84228515625, Time:33.44546937942505\n",
      "epoch: 18/50, batch: 1500, Loss: 106.86465454101562, Time:33.47515606880188\n",
      "epoch: 18/50, batch: 1600, Loss: 105.62139892578125, Time:33.46539878845215\n",
      "epoch: 18/50, batch: 1700, Loss: 109.08251190185547, Time:33.46330189704895\n",
      "Epoch 18/50, Loss: 108.65722435893848\n",
      "Val Loss: 120.7118399483817\n",
      "epoch: 19/50, batch: 0, Loss: 112.07831573486328, Time:43.63789200782776\n",
      "epoch: 19/50, batch: 100, Loss: 111.98389434814453, Time:33.46370339393616\n",
      "epoch: 19/50, batch: 200, Loss: 108.20211029052734, Time:33.45705795288086\n",
      "epoch: 19/50, batch: 300, Loss: 109.44666290283203, Time:33.50494337081909\n",
      "epoch: 19/50, batch: 400, Loss: 110.98426818847656, Time:33.46110486984253\n",
      "epoch: 19/50, batch: 500, Loss: 109.92597198486328, Time:33.475884437561035\n",
      "epoch: 19/50, batch: 600, Loss: 109.5244369506836, Time:33.454471588134766\n",
      "epoch: 19/50, batch: 700, Loss: 105.18934631347656, Time:33.46196985244751\n",
      "epoch: 19/50, batch: 800, Loss: 107.8421859741211, Time:33.48348879814148\n",
      "epoch: 19/50, batch: 900, Loss: 106.29467010498047, Time:33.46860146522522\n",
      "epoch: 19/50, batch: 1000, Loss: 112.2707290649414, Time:33.49002933502197\n",
      "epoch: 19/50, batch: 1100, Loss: 107.43138885498047, Time:33.50064563751221\n",
      "epoch: 19/50, batch: 1200, Loss: 109.78597259521484, Time:33.479984760284424\n",
      "epoch: 19/50, batch: 1300, Loss: 106.19420623779297, Time:33.48400521278381\n",
      "epoch: 19/50, batch: 1400, Loss: 108.02136993408203, Time:33.46612334251404\n",
      "epoch: 19/50, batch: 1500, Loss: 107.95516204833984, Time:33.456865549087524\n",
      "epoch: 19/50, batch: 1600, Loss: 109.84521484375, Time:33.4521369934082\n",
      "epoch: 19/50, batch: 1700, Loss: 107.24641418457031, Time:33.4709267616272\n",
      "Epoch 19/50, Loss: 108.64069675565791\n",
      "Val Loss: 119.38429117008131\n",
      "epoch: 20/50, batch: 0, Loss: 109.21954345703125, Time:43.601500511169434\n",
      "epoch: 20/50, batch: 100, Loss: 108.85212707519531, Time:33.44455885887146\n",
      "epoch: 20/50, batch: 200, Loss: 107.197509765625, Time:33.4916250705719\n",
      "epoch: 20/50, batch: 300, Loss: 107.1517105102539, Time:33.490628480911255\n",
      "epoch: 20/50, batch: 400, Loss: 103.16309356689453, Time:33.5025794506073\n",
      "epoch: 20/50, batch: 500, Loss: 105.94014739990234, Time:33.464030027389526\n",
      "epoch: 20/50, batch: 600, Loss: 106.8066635131836, Time:33.46643686294556\n",
      "epoch: 20/50, batch: 700, Loss: 108.56828308105469, Time:33.48741936683655\n",
      "epoch: 20/50, batch: 800, Loss: 104.86323547363281, Time:33.46136426925659\n",
      "epoch: 20/50, batch: 900, Loss: 107.2784423828125, Time:33.47907257080078\n",
      "epoch: 20/50, batch: 1000, Loss: 104.67545318603516, Time:33.49617552757263\n",
      "epoch: 20/50, batch: 1100, Loss: 107.47713470458984, Time:33.50360727310181\n",
      "epoch: 20/50, batch: 1200, Loss: 110.56769561767578, Time:33.49815893173218\n",
      "epoch: 20/50, batch: 1300, Loss: 107.90518188476562, Time:33.48207116127014\n",
      "epoch: 20/50, batch: 1400, Loss: 107.30027770996094, Time:33.47702383995056\n",
      "epoch: 20/50, batch: 1500, Loss: 106.23770141601562, Time:33.492669105529785\n",
      "epoch: 20/50, batch: 1600, Loss: 111.54530334472656, Time:33.48703622817993\n",
      "epoch: 20/50, batch: 1700, Loss: 99.90879821777344, Time:33.48814034461975\n",
      "Epoch 20/50, Loss: 108.48173073929838\n",
      "Val Loss: 121.06916784169722\n",
      "epoch: 21/50, batch: 0, Loss: 106.82366943359375, Time:43.756409645080566\n",
      "epoch: 21/50, batch: 100, Loss: 113.93055725097656, Time:33.476552963256836\n",
      "epoch: 21/50, batch: 200, Loss: 113.61619567871094, Time:33.472928524017334\n",
      "epoch: 21/50, batch: 300, Loss: 112.15168762207031, Time:33.46478819847107\n",
      "epoch: 21/50, batch: 400, Loss: 106.69940948486328, Time:33.453919410705566\n",
      "epoch: 21/50, batch: 500, Loss: 105.82721710205078, Time:33.452412366867065\n",
      "epoch: 21/50, batch: 600, Loss: 110.6943130493164, Time:33.456196546554565\n",
      "epoch: 21/50, batch: 700, Loss: 106.67707061767578, Time:33.45943784713745\n",
      "epoch: 21/50, batch: 800, Loss: 106.23954772949219, Time:33.47362804412842\n",
      "epoch: 21/50, batch: 900, Loss: 103.90843963623047, Time:33.49218988418579\n",
      "epoch: 21/50, batch: 1000, Loss: 111.1258773803711, Time:33.44852828979492\n",
      "epoch: 21/50, batch: 1100, Loss: 109.4830322265625, Time:33.4819769859314\n",
      "epoch: 21/50, batch: 1200, Loss: 107.44499206542969, Time:33.461509227752686\n",
      "epoch: 21/50, batch: 1300, Loss: 111.9542007446289, Time:33.451695680618286\n",
      "epoch: 21/50, batch: 1400, Loss: 108.90489196777344, Time:33.43801689147949\n",
      "epoch: 21/50, batch: 1500, Loss: 111.04871368408203, Time:33.458171129226685\n",
      "epoch: 21/50, batch: 1600, Loss: 106.63224029541016, Time:33.489047050476074\n",
      "epoch: 21/50, batch: 1700, Loss: 111.09873962402344, Time:33.475393772125244\n",
      "Epoch 21/50, Loss: 108.51141816059976\n",
      "Val Loss: 120.95208796287069\n",
      "epoch: 22/50, batch: 0, Loss: 108.1282730102539, Time:43.73576807975769\n",
      "epoch: 22/50, batch: 100, Loss: 111.48473358154297, Time:33.43273091316223\n",
      "epoch: 22/50, batch: 200, Loss: 107.81073760986328, Time:33.47202754020691\n",
      "epoch: 22/50, batch: 300, Loss: 107.78440856933594, Time:33.46057724952698\n",
      "epoch: 22/50, batch: 400, Loss: 108.59282684326172, Time:33.475181579589844\n",
      "epoch: 22/50, batch: 500, Loss: 108.33424377441406, Time:33.475994348526\n",
      "epoch: 22/50, batch: 600, Loss: 111.72412872314453, Time:33.481815338134766\n",
      "epoch: 22/50, batch: 700, Loss: 110.75725555419922, Time:33.48746681213379\n",
      "epoch: 22/50, batch: 800, Loss: 110.22895812988281, Time:33.496155977249146\n",
      "epoch: 22/50, batch: 900, Loss: 103.93740844726562, Time:33.48649597167969\n",
      "epoch: 22/50, batch: 1000, Loss: 109.37411499023438, Time:33.484970808029175\n",
      "epoch: 22/50, batch: 1100, Loss: 107.65389251708984, Time:33.49855422973633\n",
      "epoch: 22/50, batch: 1200, Loss: 109.09637451171875, Time:33.468894720077515\n",
      "epoch: 22/50, batch: 1300, Loss: 105.42269897460938, Time:33.47222590446472\n",
      "epoch: 22/50, batch: 1400, Loss: 107.59281921386719, Time:33.477577686309814\n",
      "epoch: 22/50, batch: 1500, Loss: 111.54305267333984, Time:33.47259759902954\n",
      "epoch: 22/50, batch: 1600, Loss: 109.02076721191406, Time:33.48206424713135\n",
      "epoch: 22/50, batch: 1700, Loss: 108.60562896728516, Time:33.51897358894348\n",
      "Epoch 22/50, Loss: 108.31806842618138\n",
      "Val Loss: 122.69847542898995\n",
      "epoch: 23/50, batch: 0, Loss: 111.01887512207031, Time:43.7598922252655\n",
      "epoch: 23/50, batch: 100, Loss: 104.69869995117188, Time:33.49062395095825\n",
      "epoch: 23/50, batch: 200, Loss: 107.54316711425781, Time:33.487732887268066\n",
      "epoch: 23/50, batch: 300, Loss: 105.8764419555664, Time:33.50834608078003\n",
      "epoch: 23/50, batch: 400, Loss: 106.43579864501953, Time:33.479496717453\n",
      "epoch: 23/50, batch: 500, Loss: 109.0204849243164, Time:33.48365092277527\n",
      "epoch: 23/50, batch: 600, Loss: 110.72666931152344, Time:33.48294162750244\n",
      "epoch: 23/50, batch: 700, Loss: 104.35794830322266, Time:33.52031183242798\n",
      "epoch: 23/50, batch: 800, Loss: 107.99796295166016, Time:33.46890211105347\n",
      "epoch: 23/50, batch: 900, Loss: 109.52100372314453, Time:33.46681499481201\n",
      "epoch: 23/50, batch: 1000, Loss: 112.62757873535156, Time:33.46939516067505\n",
      "epoch: 23/50, batch: 1100, Loss: 110.56147766113281, Time:33.46605443954468\n",
      "epoch: 23/50, batch: 1200, Loss: 106.56348419189453, Time:33.45717144012451\n",
      "epoch: 23/50, batch: 1300, Loss: 107.98829650878906, Time:33.45966649055481\n",
      "epoch: 23/50, batch: 1400, Loss: 106.46056365966797, Time:33.45888876914978\n",
      "epoch: 23/50, batch: 1500, Loss: 108.5411376953125, Time:33.47398090362549\n",
      "epoch: 23/50, batch: 1600, Loss: 108.0512466430664, Time:33.48181343078613\n",
      "epoch: 23/50, batch: 1700, Loss: 108.89497375488281, Time:33.48056674003601\n",
      "Epoch 23/50, Loss: 108.28062641395198\n",
      "Val Loss: 121.66440906135404\n",
      "epoch: 24/50, batch: 0, Loss: 108.41867065429688, Time:43.76128554344177\n",
      "epoch: 24/50, batch: 100, Loss: 108.46797180175781, Time:33.49473023414612\n",
      "epoch: 24/50, batch: 200, Loss: 109.79638671875, Time:33.45592260360718\n",
      "epoch: 24/50, batch: 300, Loss: 103.36476135253906, Time:33.458759784698486\n",
      "epoch: 24/50, batch: 400, Loss: 107.384033203125, Time:33.47723054885864\n",
      "epoch: 24/50, batch: 500, Loss: 103.47013092041016, Time:33.49564599990845\n",
      "epoch: 24/50, batch: 600, Loss: 106.18106079101562, Time:33.568692684173584\n",
      "epoch: 24/50, batch: 700, Loss: 103.17822265625, Time:33.624661684036255\n",
      "epoch: 24/50, batch: 800, Loss: 105.24581909179688, Time:33.47361636161804\n",
      "epoch: 24/50, batch: 900, Loss: 110.2720947265625, Time:33.45008111000061\n",
      "epoch: 24/50, batch: 1000, Loss: 110.26448822021484, Time:33.466529846191406\n",
      "epoch: 24/50, batch: 1100, Loss: 106.07009887695312, Time:33.46811270713806\n",
      "epoch: 24/50, batch: 1200, Loss: 107.12113189697266, Time:33.47160458564758\n",
      "epoch: 24/50, batch: 1300, Loss: 112.29493713378906, Time:33.469743967056274\n",
      "epoch: 24/50, batch: 1400, Loss: 110.5396499633789, Time:33.482449769973755\n",
      "epoch: 24/50, batch: 1500, Loss: 104.91641998291016, Time:33.47019338607788\n",
      "epoch: 24/50, batch: 1600, Loss: 108.04379272460938, Time:33.46381187438965\n",
      "epoch: 24/50, batch: 1700, Loss: 100.8863525390625, Time:33.46000003814697\n",
      "Epoch 24/50, Loss: 108.19623413960366\n",
      "Val Loss: 120.2008593500877\n",
      "epoch: 25/50, batch: 0, Loss: 106.73628234863281, Time:43.85652565956116\n",
      "epoch: 25/50, batch: 100, Loss: 107.23809051513672, Time:33.44458556175232\n",
      "epoch: 25/50, batch: 200, Loss: 108.42192840576172, Time:33.45788788795471\n",
      "epoch: 25/50, batch: 300, Loss: 107.11598205566406, Time:33.47798204421997\n",
      "epoch: 25/50, batch: 400, Loss: 110.52088165283203, Time:33.5203058719635\n",
      "epoch: 25/50, batch: 500, Loss: 109.2041244506836, Time:33.479575634002686\n",
      "epoch: 25/50, batch: 600, Loss: 111.2485122680664, Time:33.47487258911133\n",
      "epoch: 25/50, batch: 700, Loss: 110.66963958740234, Time:33.47892141342163\n",
      "epoch: 25/50, batch: 800, Loss: 112.16907501220703, Time:33.467570781707764\n",
      "epoch: 25/50, batch: 900, Loss: 109.6172103881836, Time:33.494624614715576\n",
      "epoch: 25/50, batch: 1000, Loss: 109.85906219482422, Time:33.461082220077515\n",
      "epoch: 25/50, batch: 1100, Loss: 107.24864959716797, Time:33.493098735809326\n",
      "epoch: 25/50, batch: 1200, Loss: 105.42881774902344, Time:33.469836950302124\n",
      "epoch: 25/50, batch: 1300, Loss: 133.83462524414062, Time:33.49377632141113\n",
      "epoch: 25/50, batch: 1400, Loss: 106.81509399414062, Time:33.50135898590088\n",
      "epoch: 25/50, batch: 1500, Loss: 107.85981750488281, Time:33.46272087097168\n",
      "epoch: 25/50, batch: 1600, Loss: 108.10437774658203, Time:33.4768168926239\n",
      "epoch: 25/50, batch: 1700, Loss: 114.85704040527344, Time:33.47012209892273\n",
      "Epoch 25/50, Loss: 108.20460969329221\n",
      "Val Loss: 118.25985222641303\n",
      "epoch: 26/50, batch: 0, Loss: 106.83600616455078, Time:43.728437185287476\n",
      "epoch: 26/50, batch: 100, Loss: 106.34326934814453, Time:33.45607399940491\n",
      "epoch: 26/50, batch: 200, Loss: 107.21114349365234, Time:33.460301876068115\n",
      "epoch: 26/50, batch: 300, Loss: 109.01519012451172, Time:33.529263973236084\n",
      "epoch: 26/50, batch: 400, Loss: 107.87213897705078, Time:33.48334622383118\n",
      "epoch: 26/50, batch: 500, Loss: 104.23493957519531, Time:33.49329614639282\n",
      "epoch: 26/50, batch: 600, Loss: 110.27223205566406, Time:33.46483564376831\n",
      "epoch: 26/50, batch: 700, Loss: 107.54710388183594, Time:33.478487491607666\n",
      "epoch: 26/50, batch: 800, Loss: 110.72015380859375, Time:33.47514748573303\n",
      "epoch: 26/50, batch: 900, Loss: 110.7146987915039, Time:33.478930711746216\n",
      "epoch: 26/50, batch: 1000, Loss: 107.76348876953125, Time:33.47602605819702\n",
      "epoch: 26/50, batch: 1100, Loss: 105.877685546875, Time:33.481724977493286\n",
      "epoch: 26/50, batch: 1200, Loss: 110.51514434814453, Time:33.535253286361694\n",
      "epoch: 26/50, batch: 1300, Loss: 112.82240295410156, Time:33.497597455978394\n",
      "epoch: 26/50, batch: 1400, Loss: 111.60501861572266, Time:33.51870322227478\n",
      "epoch: 26/50, batch: 1500, Loss: 112.19151306152344, Time:33.49614977836609\n",
      "epoch: 26/50, batch: 1600, Loss: 108.19535827636719, Time:33.47162866592407\n",
      "epoch: 26/50, batch: 1700, Loss: 108.91930389404297, Time:33.49614191055298\n",
      "Epoch 26/50, Loss: 108.0443098838828\n",
      "Val Loss: 121.97751109843351\n",
      "epoch: 27/50, batch: 0, Loss: 113.31043243408203, Time:43.7359938621521\n",
      "epoch: 27/50, batch: 100, Loss: 104.43526458740234, Time:33.48613262176514\n",
      "epoch: 27/50, batch: 200, Loss: 104.41520690917969, Time:33.49901032447815\n",
      "epoch: 27/50, batch: 300, Loss: 109.87252044677734, Time:33.489660024642944\n",
      "epoch: 27/50, batch: 400, Loss: 108.68901062011719, Time:33.47315955162048\n",
      "epoch: 27/50, batch: 500, Loss: 106.32013702392578, Time:33.477882862091064\n",
      "epoch: 27/50, batch: 600, Loss: 104.17864990234375, Time:33.4791145324707\n",
      "epoch: 27/50, batch: 700, Loss: 110.03260803222656, Time:33.466498613357544\n",
      "epoch: 27/50, batch: 800, Loss: 105.33133697509766, Time:33.46569848060608\n",
      "epoch: 27/50, batch: 900, Loss: 109.52083587646484, Time:33.47256302833557\n",
      "epoch: 27/50, batch: 1000, Loss: 107.57891845703125, Time:33.46958136558533\n",
      "epoch: 27/50, batch: 1100, Loss: 107.68215942382812, Time:33.52355742454529\n",
      "epoch: 27/50, batch: 1200, Loss: 109.85042572021484, Time:33.504680156707764\n",
      "epoch: 27/50, batch: 1300, Loss: 106.45677947998047, Time:33.48496913909912\n",
      "epoch: 27/50, batch: 1400, Loss: 107.93262481689453, Time:33.49725604057312\n",
      "epoch: 27/50, batch: 1500, Loss: 109.8084716796875, Time:33.46238589286804\n",
      "epoch: 27/50, batch: 1600, Loss: 112.46833801269531, Time:33.47743010520935\n",
      "epoch: 27/50, batch: 1700, Loss: 107.56697845458984, Time:33.51007413864136\n",
      "Epoch 27/50, Loss: 107.98636747868491\n",
      "Val Loss: 121.16185019356864\n",
      "epoch: 28/50, batch: 0, Loss: 107.28639221191406, Time:43.6743369102478\n",
      "epoch: 28/50, batch: 100, Loss: 106.55766296386719, Time:33.48479628562927\n",
      "epoch: 28/50, batch: 200, Loss: 109.69357299804688, Time:33.481180906295776\n",
      "epoch: 28/50, batch: 300, Loss: 106.06693267822266, Time:33.47372817993164\n",
      "epoch: 28/50, batch: 400, Loss: 111.67929077148438, Time:33.496394634246826\n",
      "epoch: 28/50, batch: 500, Loss: 107.02607727050781, Time:33.49739384651184\n",
      "epoch: 28/50, batch: 600, Loss: 109.50597381591797, Time:33.45739197731018\n",
      "epoch: 28/50, batch: 700, Loss: 106.3634033203125, Time:33.478575229644775\n",
      "epoch: 28/50, batch: 800, Loss: 111.34772491455078, Time:33.461012840270996\n",
      "epoch: 28/50, batch: 900, Loss: 105.48521423339844, Time:33.463409185409546\n",
      "epoch: 28/50, batch: 1000, Loss: 106.53224182128906, Time:33.535099029541016\n",
      "epoch: 28/50, batch: 1100, Loss: 110.0811538696289, Time:33.480512619018555\n",
      "epoch: 28/50, batch: 1200, Loss: 103.98854064941406, Time:33.484270095825195\n",
      "epoch: 28/50, batch: 1300, Loss: 109.47163391113281, Time:33.49081611633301\n",
      "epoch: 28/50, batch: 1400, Loss: 106.47352600097656, Time:33.496761083602905\n",
      "epoch: 28/50, batch: 1500, Loss: 107.56018829345703, Time:33.47514891624451\n",
      "epoch: 28/50, batch: 1600, Loss: 111.03180694580078, Time:33.47936987876892\n",
      "epoch: 28/50, batch: 1700, Loss: 108.65367889404297, Time:33.469526052474976\n",
      "Epoch 28/50, Loss: 107.95998056843493\n",
      "Val Loss: 120.6285752588389\n",
      "epoch: 29/50, batch: 0, Loss: 107.94084930419922, Time:43.67013716697693\n",
      "epoch: 29/50, batch: 100, Loss: 104.61370849609375, Time:33.4784255027771\n",
      "epoch: 29/50, batch: 200, Loss: 107.02367401123047, Time:33.50659441947937\n",
      "epoch: 29/50, batch: 300, Loss: 109.38981628417969, Time:33.48737406730652\n",
      "epoch: 29/50, batch: 400, Loss: 104.91751861572266, Time:33.477051973342896\n",
      "epoch: 29/50, batch: 500, Loss: 104.94400024414062, Time:33.48575472831726\n",
      "epoch: 29/50, batch: 600, Loss: 108.54492950439453, Time:33.46352672576904\n",
      "epoch: 29/50, batch: 700, Loss: 111.33087921142578, Time:33.48412871360779\n",
      "epoch: 29/50, batch: 800, Loss: 110.84077453613281, Time:33.488271713256836\n",
      "epoch: 29/50, batch: 900, Loss: 108.55296325683594, Time:33.47548770904541\n",
      "epoch: 29/50, batch: 1000, Loss: 110.16101837158203, Time:33.48810696601868\n",
      "epoch: 29/50, batch: 1100, Loss: 109.62889862060547, Time:33.47631120681763\n",
      "epoch: 29/50, batch: 1200, Loss: 105.41300964355469, Time:33.50857615470886\n",
      "epoch: 29/50, batch: 1300, Loss: 111.74446105957031, Time:33.47653770446777\n",
      "epoch: 29/50, batch: 1400, Loss: 107.46060180664062, Time:33.47870683670044\n",
      "epoch: 29/50, batch: 1500, Loss: 109.71637725830078, Time:33.507736921310425\n",
      "epoch: 29/50, batch: 1600, Loss: 111.31356048583984, Time:33.49683666229248\n",
      "epoch: 29/50, batch: 1700, Loss: 105.5472183227539, Time:33.49108648300171\n",
      "Epoch 29/50, Loss: 107.86778804046719\n",
      "Val Loss: 121.39616057726802\n",
      "epoch: 30/50, batch: 0, Loss: 108.28665161132812, Time:43.780195236206055\n",
      "epoch: 30/50, batch: 100, Loss: 107.90373229980469, Time:33.4463906288147\n",
      "epoch: 30/50, batch: 200, Loss: 106.47175598144531, Time:33.52846646308899\n",
      "epoch: 30/50, batch: 300, Loss: 108.07905578613281, Time:33.46790885925293\n",
      "epoch: 30/50, batch: 400, Loss: 107.5487289428711, Time:33.474956035614014\n",
      "epoch: 30/50, batch: 500, Loss: 107.41853332519531, Time:33.483110427856445\n",
      "epoch: 30/50, batch: 600, Loss: 112.19383239746094, Time:33.485695123672485\n",
      "epoch: 30/50, batch: 700, Loss: 111.71194458007812, Time:34.559728145599365\n",
      "epoch: 30/50, batch: 800, Loss: 102.42552185058594, Time:34.02218270301819\n",
      "epoch: 30/50, batch: 900, Loss: 108.1781997680664, Time:34.484633445739746\n",
      "epoch: 30/50, batch: 1000, Loss: 104.49879455566406, Time:34.157212257385254\n",
      "epoch: 30/50, batch: 1100, Loss: 110.85781860351562, Time:34.32725262641907\n",
      "epoch: 30/50, batch: 1200, Loss: 106.58949279785156, Time:34.30952286720276\n",
      "epoch: 30/50, batch: 1300, Loss: 107.27152252197266, Time:34.49048662185669\n",
      "epoch: 30/50, batch: 1400, Loss: 104.69100952148438, Time:33.929699420928955\n",
      "epoch: 30/50, batch: 1500, Loss: 109.02534484863281, Time:34.43797516822815\n",
      "epoch: 30/50, batch: 1600, Loss: 107.3792953491211, Time:34.33703541755676\n",
      "epoch: 30/50, batch: 1700, Loss: 105.9309310913086, Time:34.42025947570801\n",
      "Epoch 30/50, Loss: 107.78544709825925\n",
      "Val Loss: 120.22156337815888\n",
      "epoch: 31/50, batch: 0, Loss: 104.41917419433594, Time:44.62914061546326\n",
      "epoch: 31/50, batch: 100, Loss: 105.04926300048828, Time:34.12293863296509\n",
      "epoch: 31/50, batch: 200, Loss: 111.47608184814453, Time:33.881150245666504\n",
      "epoch: 31/50, batch: 300, Loss: 109.64697265625, Time:33.96949791908264\n",
      "epoch: 31/50, batch: 400, Loss: 108.09050750732422, Time:34.28100228309631\n",
      "epoch: 31/50, batch: 500, Loss: 106.05855560302734, Time:34.02509117126465\n",
      "epoch: 31/50, batch: 600, Loss: 108.13895416259766, Time:34.27025651931763\n",
      "epoch: 31/50, batch: 700, Loss: 102.64435577392578, Time:34.118067264556885\n",
      "epoch: 31/50, batch: 800, Loss: 108.52737426757812, Time:34.63609957695007\n",
      "epoch: 31/50, batch: 900, Loss: 106.8875503540039, Time:34.48853421211243\n",
      "epoch: 31/50, batch: 1000, Loss: 110.86650848388672, Time:34.840686082839966\n",
      "epoch: 31/50, batch: 1100, Loss: 106.21005249023438, Time:34.36367464065552\n",
      "epoch: 31/50, batch: 1200, Loss: 107.60746765136719, Time:33.98841428756714\n",
      "epoch: 31/50, batch: 1300, Loss: 106.98352813720703, Time:33.96594548225403\n",
      "epoch: 31/50, batch: 1400, Loss: 106.5686264038086, Time:33.95512247085571\n",
      "epoch: 31/50, batch: 1500, Loss: 106.78716278076172, Time:33.99884915351868\n",
      "epoch: 31/50, batch: 1600, Loss: 109.21370697021484, Time:34.00599932670593\n",
      "epoch: 31/50, batch: 1700, Loss: 111.13713836669922, Time:33.970414876937866\n",
      "Epoch 31/50, Loss: 107.76658333939604\n",
      "Val Loss: 122.61621825549068\n",
      "epoch: 32/50, batch: 0, Loss: 112.60486602783203, Time:44.309175968170166\n",
      "epoch: 32/50, batch: 100, Loss: 109.66468048095703, Time:33.93877983093262\n",
      "epoch: 32/50, batch: 200, Loss: 107.59313201904297, Time:33.924389600753784\n",
      "epoch: 32/50, batch: 300, Loss: 104.65402221679688, Time:34.005876541137695\n",
      "epoch: 32/50, batch: 400, Loss: 108.18904113769531, Time:33.916358947753906\n",
      "epoch: 32/50, batch: 500, Loss: 111.39611053466797, Time:33.97235631942749\n",
      "epoch: 32/50, batch: 600, Loss: 108.79948425292969, Time:33.9657621383667\n",
      "epoch: 32/50, batch: 700, Loss: 109.26667022705078, Time:33.95001220703125\n",
      "epoch: 32/50, batch: 800, Loss: 107.99042510986328, Time:33.982882261276245\n",
      "epoch: 32/50, batch: 900, Loss: 101.45587158203125, Time:33.966771364212036\n",
      "epoch: 32/50, batch: 1000, Loss: 107.67251586914062, Time:33.95574951171875\n",
      "epoch: 32/50, batch: 1100, Loss: 104.12015533447266, Time:33.96202993392944\n",
      "epoch: 32/50, batch: 1200, Loss: 108.9305419921875, Time:33.97210335731506\n",
      "epoch: 32/50, batch: 1300, Loss: 106.52906799316406, Time:33.97873258590698\n",
      "epoch: 32/50, batch: 1400, Loss: 111.97854614257812, Time:33.9725546836853\n",
      "epoch: 32/50, batch: 1500, Loss: 103.8790283203125, Time:33.99432873725891\n",
      "epoch: 32/50, batch: 1600, Loss: 108.7226333618164, Time:34.002115964889526\n",
      "epoch: 32/50, batch: 1700, Loss: 106.3785629272461, Time:33.95474934577942\n",
      "Epoch 32/50, Loss: 107.71085622618055\n",
      "Val Loss: 120.96719774518694\n",
      "epoch: 33/50, batch: 0, Loss: 113.03438568115234, Time:44.79798650741577\n",
      "epoch: 33/50, batch: 100, Loss: 108.5951919555664, Time:34.214130878448486\n",
      "epoch: 33/50, batch: 200, Loss: 103.41278076171875, Time:34.527825117111206\n",
      "epoch: 33/50, batch: 300, Loss: 108.97579956054688, Time:33.84707713127136\n",
      "epoch: 33/50, batch: 400, Loss: 99.00809478759766, Time:33.26080918312073\n",
      "epoch: 33/50, batch: 500, Loss: 111.34117889404297, Time:33.3101646900177\n",
      "epoch: 33/50, batch: 600, Loss: 100.82535552978516, Time:33.73751711845398\n",
      "epoch: 33/50, batch: 700, Loss: 112.79269409179688, Time:33.74213981628418\n",
      "epoch: 33/50, batch: 800, Loss: 109.24207305908203, Time:33.53042769432068\n",
      "epoch: 33/50, batch: 900, Loss: 104.83648681640625, Time:33.47248983383179\n",
      "epoch: 33/50, batch: 1000, Loss: 105.83391571044922, Time:33.52408957481384\n",
      "epoch: 33/50, batch: 1100, Loss: 107.50652313232422, Time:33.48483061790466\n",
      "epoch: 33/50, batch: 1200, Loss: 108.99332427978516, Time:34.25100111961365\n",
      "epoch: 33/50, batch: 1300, Loss: 107.41688537597656, Time:33.81421732902527\n",
      "epoch: 33/50, batch: 1400, Loss: 109.233642578125, Time:33.942816495895386\n",
      "epoch: 33/50, batch: 1500, Loss: 108.71373748779297, Time:33.9484920501709\n",
      "epoch: 33/50, batch: 1600, Loss: 107.3370361328125, Time:34.51947069168091\n",
      "epoch: 33/50, batch: 1700, Loss: 103.3714828491211, Time:33.91278004646301\n",
      "Epoch 33/50, Loss: 107.6625728312058\n",
      "Val Loss: 122.4492990299147\n",
      "epoch: 34/50, batch: 0, Loss: 108.26763153076172, Time:41.50609827041626\n",
      "epoch: 34/50, batch: 100, Loss: 106.26580047607422, Time:33.850308895111084\n",
      "epoch: 34/50, batch: 200, Loss: 111.69287109375, Time:33.90374755859375\n",
      "epoch: 34/50, batch: 300, Loss: 110.50646209716797, Time:33.9331796169281\n",
      "epoch: 34/50, batch: 400, Loss: 105.11895751953125, Time:34.571802616119385\n",
      "epoch: 34/50, batch: 500, Loss: 107.31842041015625, Time:33.941365480422974\n",
      "epoch: 34/50, batch: 600, Loss: 107.58150482177734, Time:33.974785566329956\n",
      "epoch: 34/50, batch: 700, Loss: 108.4764404296875, Time:33.99429440498352\n",
      "epoch: 34/50, batch: 800, Loss: 105.69165802001953, Time:34.0512797832489\n",
      "epoch: 34/50, batch: 900, Loss: 118.2321548461914, Time:33.998719215393066\n",
      "epoch: 34/50, batch: 1000, Loss: 114.08061218261719, Time:33.97044491767883\n",
      "epoch: 34/50, batch: 1100, Loss: 101.4961929321289, Time:33.969969511032104\n",
      "epoch: 34/50, batch: 1200, Loss: 106.71361541748047, Time:33.960959672927856\n",
      "epoch: 34/50, batch: 1300, Loss: 104.2472915649414, Time:34.0268874168396\n",
      "epoch: 34/50, batch: 1400, Loss: 111.69951629638672, Time:33.96358680725098\n",
      "epoch: 34/50, batch: 1500, Loss: 105.63019561767578, Time:34.3318772315979\n",
      "epoch: 34/50, batch: 1600, Loss: 110.94279479980469, Time:34.10834503173828\n",
      "epoch: 34/50, batch: 1700, Loss: 112.7014389038086, Time:34.00702786445618\n",
      "Epoch 34/50, Loss: 107.61143263677471\n",
      "Val Loss: 120.9062823237205\n",
      "epoch: 35/50, batch: 0, Loss: 105.85249328613281, Time:44.30126476287842\n",
      "epoch: 35/50, batch: 100, Loss: 109.63558959960938, Time:34.130619049072266\n",
      "epoch: 35/50, batch: 200, Loss: 108.6581039428711, Time:34.08288764953613\n",
      "epoch: 35/50, batch: 300, Loss: 109.08998107910156, Time:34.07723546028137\n",
      "epoch: 35/50, batch: 400, Loss: 108.15623474121094, Time:33.7632794380188\n",
      "epoch: 35/50, batch: 500, Loss: 106.08001708984375, Time:34.12855386734009\n",
      "epoch: 35/50, batch: 600, Loss: 104.58443450927734, Time:33.918861865997314\n",
      "epoch: 35/50, batch: 700, Loss: 107.20729064941406, Time:34.077545404434204\n",
      "epoch: 35/50, batch: 800, Loss: 109.890869140625, Time:34.02769494056702\n",
      "epoch: 35/50, batch: 900, Loss: 106.8695297241211, Time:33.99568581581116\n",
      "epoch: 35/50, batch: 1000, Loss: 105.3524398803711, Time:34.021775245666504\n",
      "epoch: 35/50, batch: 1100, Loss: 109.18551635742188, Time:33.98595595359802\n",
      "epoch: 35/50, batch: 1200, Loss: 107.2791748046875, Time:34.082910776138306\n",
      "epoch: 35/50, batch: 1300, Loss: 108.32243347167969, Time:34.16513442993164\n",
      "epoch: 35/50, batch: 1400, Loss: 107.72872924804688, Time:34.265326261520386\n",
      "epoch: 35/50, batch: 1500, Loss: 111.02213287353516, Time:34.70422601699829\n",
      "epoch: 35/50, batch: 1600, Loss: 111.64002227783203, Time:33.806822299957275\n",
      "epoch: 35/50, batch: 1700, Loss: 105.74134826660156, Time:34.078038930892944\n",
      "Epoch 35/50, Loss: 107.5676078206147\n",
      "Val Loss: 120.78539615553252\n",
      "epoch: 36/50, batch: 0, Loss: 106.99449920654297, Time:44.05130743980408\n",
      "epoch: 36/50, batch: 100, Loss: 104.29724884033203, Time:33.84435677528381\n",
      "epoch: 36/50, batch: 200, Loss: 106.78678131103516, Time:33.962810754776\n",
      "epoch: 36/50, batch: 300, Loss: 109.95096588134766, Time:33.93531918525696\n",
      "epoch: 36/50, batch: 400, Loss: 110.61306762695312, Time:33.92944288253784\n",
      "epoch: 36/50, batch: 500, Loss: 111.72793579101562, Time:33.92317771911621\n",
      "epoch: 36/50, batch: 600, Loss: 109.60935974121094, Time:33.877856969833374\n",
      "epoch: 36/50, batch: 700, Loss: 104.81776428222656, Time:33.90247321128845\n",
      "epoch: 36/50, batch: 800, Loss: 106.06668090820312, Time:34.03245735168457\n",
      "epoch: 36/50, batch: 900, Loss: 106.60992431640625, Time:34.06103205680847\n",
      "epoch: 36/50, batch: 1000, Loss: 109.87902069091797, Time:34.24108147621155\n",
      "epoch: 36/50, batch: 1100, Loss: 107.06080627441406, Time:34.55897092819214\n",
      "epoch: 36/50, batch: 1200, Loss: 106.2540283203125, Time:34.05405902862549\n",
      "epoch: 36/50, batch: 1300, Loss: 108.56832885742188, Time:34.261882066726685\n",
      "epoch: 36/50, batch: 1400, Loss: 105.24211120605469, Time:34.30208134651184\n",
      "epoch: 36/50, batch: 1500, Loss: 109.1590347290039, Time:33.96312499046326\n",
      "epoch: 36/50, batch: 1600, Loss: 104.26643371582031, Time:34.08810520172119\n",
      "epoch: 36/50, batch: 1700, Loss: 104.74608612060547, Time:34.02765679359436\n",
      "Epoch 36/50, Loss: 107.50907354491488\n",
      "Val Loss: 124.31420892209422\n",
      "epoch: 37/50, batch: 0, Loss: 110.6746826171875, Time:44.48638105392456\n",
      "epoch: 37/50, batch: 100, Loss: 108.57380676269531, Time:33.88490676879883\n",
      "epoch: 37/50, batch: 200, Loss: 106.21607971191406, Time:33.92787837982178\n",
      "epoch: 37/50, batch: 300, Loss: 104.73536682128906, Time:34.15612244606018\n",
      "epoch: 37/50, batch: 400, Loss: 106.0980453491211, Time:34.332539081573486\n",
      "epoch: 37/50, batch: 500, Loss: 107.8463363647461, Time:34.514870405197144\n",
      "epoch: 37/50, batch: 600, Loss: 107.40701293945312, Time:34.348150968551636\n",
      "epoch: 37/50, batch: 700, Loss: 105.09622192382812, Time:34.36271643638611\n",
      "epoch: 37/50, batch: 800, Loss: 104.52889251708984, Time:34.43850016593933\n",
      "epoch: 37/50, batch: 900, Loss: 103.43241882324219, Time:34.11369299888611\n",
      "epoch: 37/50, batch: 1000, Loss: 107.82360076904297, Time:34.05391550064087\n",
      "epoch: 37/50, batch: 1100, Loss: 106.219970703125, Time:34.152111291885376\n",
      "epoch: 37/50, batch: 1200, Loss: 107.7197494506836, Time:34.00462484359741\n",
      "epoch: 37/50, batch: 1300, Loss: 104.55669403076172, Time:33.978034257888794\n",
      "epoch: 37/50, batch: 1400, Loss: 110.62232971191406, Time:34.30834221839905\n",
      "epoch: 37/50, batch: 1500, Loss: 108.55622863769531, Time:34.137261390686035\n",
      "epoch: 37/50, batch: 1600, Loss: 108.22258758544922, Time:33.94540786743164\n",
      "epoch: 37/50, batch: 1700, Loss: 109.69576263427734, Time:34.14170432090759\n",
      "Epoch 37/50, Loss: 107.49183452354802\n",
      "Val Loss: 122.62545795051419\n",
      "epoch: 38/50, batch: 0, Loss: 109.61349487304688, Time:41.7100613117218\n",
      "epoch: 38/50, batch: 100, Loss: 111.01654815673828, Time:34.58402967453003\n",
      "epoch: 38/50, batch: 200, Loss: 111.37625122070312, Time:34.27150559425354\n",
      "epoch: 38/50, batch: 300, Loss: 107.74668884277344, Time:34.925901889801025\n",
      "epoch: 38/50, batch: 400, Loss: 111.81147766113281, Time:34.747520208358765\n",
      "epoch: 38/50, batch: 500, Loss: 108.21080780029297, Time:35.08462929725647\n",
      "epoch: 38/50, batch: 600, Loss: 108.19506072998047, Time:34.78330397605896\n",
      "epoch: 38/50, batch: 700, Loss: 109.10821533203125, Time:34.13128852844238\n",
      "epoch: 38/50, batch: 800, Loss: 111.70323944091797, Time:34.201796531677246\n",
      "epoch: 38/50, batch: 900, Loss: 108.85933685302734, Time:34.219573736190796\n",
      "epoch: 38/50, batch: 1000, Loss: 105.50343322753906, Time:33.764785051345825\n",
      "epoch: 38/50, batch: 1100, Loss: 105.56340026855469, Time:33.79625463485718\n",
      "epoch: 38/50, batch: 1200, Loss: 108.20899963378906, Time:33.78497576713562\n",
      "epoch: 38/50, batch: 1300, Loss: 109.0518569946289, Time:33.81071424484253\n",
      "epoch: 38/50, batch: 1400, Loss: 109.36698150634766, Time:33.660526275634766\n",
      "epoch: 38/50, batch: 1500, Loss: 108.56329345703125, Time:33.91636300086975\n",
      "epoch: 38/50, batch: 1600, Loss: 105.69341278076172, Time:33.815805196762085\n",
      "epoch: 38/50, batch: 1700, Loss: 104.36265563964844, Time:33.777482748031616\n",
      "Epoch 38/50, Loss: 107.42808729461407\n",
      "Val Loss: 121.75447913967834\n",
      "epoch: 39/50, batch: 0, Loss: 108.18514251708984, Time:41.4429247379303\n",
      "epoch: 39/50, batch: 100, Loss: 108.20071411132812, Time:33.865734815597534\n",
      "epoch: 39/50, batch: 200, Loss: 109.08670043945312, Time:33.79609727859497\n",
      "epoch: 39/50, batch: 300, Loss: 109.42737579345703, Time:33.79167652130127\n",
      "epoch: 39/50, batch: 400, Loss: 105.43133544921875, Time:33.76055979728699\n",
      "epoch: 39/50, batch: 500, Loss: 117.43973541259766, Time:33.80186700820923\n",
      "epoch: 39/50, batch: 600, Loss: 107.4156494140625, Time:33.794540882110596\n",
      "epoch: 39/50, batch: 700, Loss: 109.00617980957031, Time:33.784175634384155\n",
      "epoch: 39/50, batch: 800, Loss: 108.03285217285156, Time:33.77818751335144\n",
      "epoch: 39/50, batch: 900, Loss: 106.61214447021484, Time:34.0191113948822\n",
      "epoch: 39/50, batch: 1000, Loss: 104.57325744628906, Time:33.9712917804718\n",
      "epoch: 39/50, batch: 1100, Loss: 106.30335235595703, Time:33.81841540336609\n",
      "epoch: 39/50, batch: 1200, Loss: 104.67289733886719, Time:33.78969168663025\n",
      "epoch: 39/50, batch: 1300, Loss: 107.4351806640625, Time:33.797874212265015\n",
      "epoch: 39/50, batch: 1400, Loss: 109.74939727783203, Time:33.81308579444885\n",
      "epoch: 39/50, batch: 1500, Loss: 103.92338562011719, Time:33.7906711101532\n",
      "epoch: 39/50, batch: 1600, Loss: 107.09347534179688, Time:33.805307388305664\n",
      "epoch: 39/50, batch: 1700, Loss: 128.24179077148438, Time:33.819535970687866\n",
      "Epoch 39/50, Loss: 107.79176652793556\n",
      "Val Loss: 123.00682444669762\n",
      "epoch: 40/50, batch: 0, Loss: 105.86966705322266, Time:41.40859389305115\n",
      "epoch: 40/50, batch: 100, Loss: 105.36466979980469, Time:33.77582120895386\n",
      "epoch: 40/50, batch: 200, Loss: 103.98382568359375, Time:33.789878606796265\n",
      "epoch: 40/50, batch: 300, Loss: 107.74266052246094, Time:33.84219145774841\n",
      "epoch: 40/50, batch: 400, Loss: 106.86355590820312, Time:34.01772165298462\n",
      "epoch: 40/50, batch: 500, Loss: 109.60833740234375, Time:34.3487331867218\n",
      "epoch: 40/50, batch: 600, Loss: 108.43415832519531, Time:33.79574155807495\n",
      "epoch: 40/50, batch: 700, Loss: 105.07804870605469, Time:34.0195791721344\n",
      "epoch: 40/50, batch: 800, Loss: 107.30343627929688, Time:33.86624526977539\n",
      "epoch: 40/50, batch: 900, Loss: 111.09627532958984, Time:34.13543343544006\n",
      "epoch: 40/50, batch: 1000, Loss: 113.82868957519531, Time:33.87711453437805\n",
      "epoch: 40/50, batch: 1100, Loss: 104.23199462890625, Time:33.676886558532715\n",
      "epoch: 40/50, batch: 1200, Loss: 110.27271270751953, Time:33.79692625999451\n",
      "epoch: 40/50, batch: 1300, Loss: 107.7206039428711, Time:34.506205320358276\n",
      "epoch: 40/50, batch: 1400, Loss: 106.541748046875, Time:34.46386241912842\n",
      "epoch: 40/50, batch: 1500, Loss: 109.6626968383789, Time:34.334969997406006\n",
      "epoch: 40/50, batch: 1600, Loss: 108.01844024658203, Time:34.233012676239014\n",
      "epoch: 40/50, batch: 1700, Loss: 103.70852661132812, Time:33.9655487537384\n",
      "Epoch 40/50, Loss: 107.36627221749642\n",
      "Val Loss: 122.86388559146803\n",
      "epoch: 41/50, batch: 0, Loss: 108.71427917480469, Time:41.78457522392273\n",
      "epoch: 41/50, batch: 100, Loss: 108.92233276367188, Time:34.33842658996582\n",
      "epoch: 41/50, batch: 200, Loss: 106.78057861328125, Time:34.2138454914093\n",
      "epoch: 41/50, batch: 300, Loss: 108.90238952636719, Time:34.13530993461609\n",
      "epoch: 41/50, batch: 400, Loss: 100.23766326904297, Time:34.027708292007446\n",
      "epoch: 41/50, batch: 500, Loss: 109.13447570800781, Time:33.695995569229126\n",
      "epoch: 41/50, batch: 600, Loss: 109.1418228149414, Time:33.866042613983154\n",
      "epoch: 41/50, batch: 700, Loss: 107.62842559814453, Time:33.85085701942444\n",
      "epoch: 41/50, batch: 800, Loss: 108.46414947509766, Time:33.9308385848999\n",
      "epoch: 41/50, batch: 900, Loss: 109.25106048583984, Time:34.11641502380371\n",
      "epoch: 41/50, batch: 1000, Loss: 107.33841705322266, Time:34.83631634712219\n",
      "epoch: 41/50, batch: 1100, Loss: 106.01777648925781, Time:34.663575410842896\n",
      "epoch: 41/50, batch: 1200, Loss: 104.9429702758789, Time:34.77211403846741\n",
      "epoch: 41/50, batch: 1300, Loss: 106.81693267822266, Time:34.64143753051758\n",
      "epoch: 41/50, batch: 1400, Loss: 107.38199615478516, Time:34.62784028053284\n",
      "epoch: 41/50, batch: 1500, Loss: 107.44158172607422, Time:34.46298027038574\n",
      "epoch: 41/50, batch: 1600, Loss: 110.23017883300781, Time:34.63093328475952\n",
      "epoch: 41/50, batch: 1700, Loss: 108.46366119384766, Time:34.57216954231262\n",
      "Epoch 41/50, Loss: 107.30065205308975\n",
      "Val Loss: 122.28472290039062\n",
      "epoch: 42/50, batch: 0, Loss: 105.4398422241211, Time:41.82114768028259\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 29\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, batch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Time:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, num_epochs, i, loss\u001b[38;5;241m.\u001b[39mdata, time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mtime_start))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### 开始训练\n",
    "\n",
    "import time\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "num_epochs = 10\n",
    "time_start = time.time()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(device)#.half()\n",
    "for epoch in range(num_epochs):\n",
    "    # train\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for i, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "        # print(batch_data.shape)\n",
    "        # print(batch_labels.shape)\n",
    "        batch_data = batch_data.to(device)#.float()\n",
    "        batch_labels = batch_labels.to(device)#.float()\n",
    "\n",
    "        out = model(batch_data)#.unsqueeze(1)\n",
    "        # out = model(batch_data).permute(0, 2, 1).unsqueeze(1)\n",
    "        # print(out.shape)\n",
    "        # print(batch_labels.shape)\n",
    "        loss = criterion(out, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch: {}/{}, batch: {}, Loss: {}, Time:{}\".format(epoch+1, num_epochs, i, loss.data, time.time()-time_start))\n",
    "            save_img(batch_data, out, batch_labels=batch_labels, num=i, output_dir='output_images_train')    \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_loss = running_loss/len(train_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_loss}')\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_data, batch_labels) in enumerate(val_loader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            out = model(batch_data)#.unsqueeze(1)\n",
    "            # out = model(batch_data).permute(0, 2, 1).unsqueeze(1)\n",
    "            loss = criterion(out, batch_labels)\n",
    "            val_loss += loss.item()\n",
    "    val_loss_list.append(val_loss/len(val_loader))\n",
    "    print(f'Val Loss: {val_loss/len(val_loader)}')\n",
    "    torch.save(model, \"./CNNLSTM_{}.pth\".format(epoch+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 绘制loss曲线图\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(train_loss_list,'s-',color = 'r',label=\"Train\")\n",
    "plt.plot(val_loss_list,'o-',color = 'g',label=\"Val\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 加载模型 如果没有训练\n",
    "model = torch.load(\"CNNLSTM_50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4990/4990 [04:09<00:00, 20.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:\n",
      "MSE: 123.0371238662628\n",
      "RMSE: 11.090368832393478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### 测试\n",
    "\n",
    "# 设置测试集范围 [0, 1] 是0% 到 100%\n",
    "test_range = [0, 1]\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "is_save_imgs = False   # 是否保存图片\n",
    "data_num, height, width = label_array.shape\n",
    "num_s = int(data_num*test_range[0])\n",
    "num_e = int(data_num*test_range[1])\n",
    "test_dataset = CustomDataset(data_array[num_s : num_e], label_array[num_s : num_e])\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "seq_test = len(test_loader)\n",
    "\n",
    "if is_save_imgs:\n",
    "    import os\n",
    "    output_dir = 'output_images_test'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# test\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "rmse = 0\n",
    "outList = np.zeros([seq_test+length, height, width])\n",
    "criterion = torch.nn.MSELoss()\n",
    "with torch.no_grad():\n",
    "    for i, (batch_data, batch_labels) in enumerate(tqdm(test_loader)):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        out = model(batch_data)\n",
    "        if i == 0:\n",
    "            outList[i:i+10] = out[0].detach().squeeze().cpu().numpy()\n",
    "        else:\n",
    "            outList[i+10] = out[0][-1].detach().squeeze().cpu().numpy()\n",
    "        loss = criterion(out, batch_labels)\n",
    "        rmse += math.sqrt(loss.item())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # 保存图片\n",
    "        if is_save_imgs: save_img(batch_data, out, batch_labels=batch_labels, num=i, output_dir=output_dir)           \n",
    "    \n",
    "print(f'Test Loss:\\nMSE: {test_loss/len(test_loader)}\\nRMSE: {rmse/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 保存测试数据到 .mat 文件中\n",
    "import scipy\n",
    "outList = outList.transpose(1,2,0)\n",
    "scipy.io.savemat('outList.mat', {'ImgBuf3': outList})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
