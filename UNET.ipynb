{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "data_name = 'PBS45p_q10_5000_SVD_60_80dB_ensemble1'\n",
    "data_name = 'PBS45p_q10'\n",
    "data_path = '\\\\\\\\ls710dc3b\\\\share\\\\OmuratoWang\\\\'\n",
    "with h5py.File(data_path+data_name+'.mat', 'r') as file:\n",
    "    # datas = file['Acq']['Data'][:]\n",
    "    datas = file['ImgBuf3'][:]\n",
    "    print(type(datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 192, 1216)\n",
      "[[[ 0.0000e+00  5.0000e+00  1.5000e+01 ...  1.1000e+02  6.3500e+02\n",
      "   -2.4434e+04]\n",
      "  [ 0.0000e+00  5.0000e+00  4.0000e+00 ...  1.5800e+02 -3.5200e+02\n",
      "    9.6000e+02]\n",
      "  [ 1.0752e+04  5.0000e+00  6.0000e+00 ... -5.7600e+02  2.2300e+02\n",
      "   -1.5732e+04]\n",
      "  ...\n",
      "  [ 0.0000e+00 -3.0000e+00 -8.0000e+00 ... -2.9700e+02 -1.6800e+02\n",
      "   -2.3718e+04]\n",
      "  [ 0.0000e+00  3.0000e+00  4.0000e+00 ...  3.8300e+02  5.2000e+01\n",
      "   -1.7826e+04]\n",
      "  [ 0.0000e+00  0.0000e+00  2.0000e+00 ...  1.2200e+02  7.1000e+02\n",
      "   -1.4315e+04]]\n",
      "\n",
      " [[ 0.0000e+00  4.0000e+00  1.2000e+01 ...  1.1600e+02  6.4800e+02\n",
      "   -1.9838e+04]\n",
      "  [ 0.0000e+00  7.0000e+00  7.0000e+00 ...  1.3200e+02 -4.0200e+02\n",
      "   -3.1614e+04]\n",
      "  [ 1.1008e+04  6.0000e+00  1.0000e+01 ... -6.1200e+02  2.3500e+02\n",
      "   -1.1030e+04]\n",
      "  ...\n",
      "  [ 0.0000e+00 -6.0000e+00 -1.3000e+01 ... -2.8900e+02 -1.7400e+02\n",
      "    1.5266e+04]\n",
      "  [ 0.0000e+00  2.0000e+00  3.0000e+00 ...  3.9100e+02  7.2000e+01\n",
      "   -1.5172e+04]\n",
      "  [ 0.0000e+00 -2.0000e+00  2.0000e+00 ...  1.2700e+02  7.2500e+02\n",
      "   -1.1357e+04]]\n",
      "\n",
      " [[ 0.0000e+00  5.0000e+00  1.2000e+01 ...  1.5200e+02  6.5900e+02\n",
      "   -1.6800e+04]\n",
      "  [ 0.0000e+00  5.0000e+00  3.0000e+00 ...  1.6300e+02 -3.9700e+02\n",
      "    1.0310e+03]\n",
      "  [ 1.1264e+04  4.0000e+00  8.0000e+00 ... -5.9300e+02  2.3500e+02\n",
      "    2.8643e+04]\n",
      "  ...\n",
      "  [ 0.0000e+00 -4.0000e+00 -1.1000e+01 ... -2.6900e+02 -1.4300e+02\n",
      "    2.5070e+04]\n",
      "  [ 0.0000e+00  2.0000e+00  3.0000e+00 ...  4.0800e+02  6.8000e+01\n",
      "   -2.8080e+03]\n",
      "  [ 0.0000e+00  0.0000e+00  3.0000e+00 ...  1.4500e+02  6.9800e+02\n",
      "    4.5650e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.0000e+00  7.0000e+00  1.6000e+01 ...  7.3000e+01  6.5000e+02\n",
      "   -1.9519e+04]\n",
      "  [ 0.0000e+00  8.0000e+00  7.0000e+00 ...  2.0800e+02 -3.2900e+02\n",
      "    1.8459e+04]\n",
      "  [-2.0480e+04  5.0000e+00  7.0000e+00 ... -6.5700e+02  1.6000e+02\n",
      "   -2.0717e+04]\n",
      "  ...\n",
      "  [ 0.0000e+00 -4.0000e+00 -8.0000e+00 ... -2.7700e+02 -1.8100e+02\n",
      "   -4.2600e+03]\n",
      "  [ 0.0000e+00  2.0000e+00  3.0000e+00 ...  4.0600e+02  4.7000e+01\n",
      "    2.9526e+04]\n",
      "  [ 0.0000e+00  1.0000e+00  4.0000e+00 ...  9.1000e+01  7.0700e+02\n",
      "    2.3190e+04]]\n",
      "\n",
      " [[ 0.0000e+00  5.0000e+00  1.3000e+01 ...  9.8000e+01  6.1900e+02\n",
      "    5.6510e+03]\n",
      "  [ 0.0000e+00  5.0000e+00  7.0000e+00 ...  2.2500e+02 -3.3700e+02\n",
      "    1.1180e+03]\n",
      "  [-2.0224e+04  5.0000e+00  7.0000e+00 ... -5.9300e+02  1.7000e+02\n",
      "    2.1640e+03]\n",
      "  ...\n",
      "  [ 0.0000e+00 -4.0000e+00 -1.0000e+01 ... -2.6300e+02 -1.7900e+02\n",
      "   -3.9280e+03]\n",
      "  [ 0.0000e+00  3.0000e+00  4.0000e+00 ...  4.2700e+02  8.7000e+01\n",
      "    1.2660e+03]\n",
      "  [ 0.0000e+00 -2.0000e+00  1.0000e+00 ...  1.0800e+02  7.1200e+02\n",
      "   -3.0683e+04]]\n",
      "\n",
      " [[ 0.0000e+00  6.0000e+00  1.4000e+01 ...  8.8000e+01  6.4300e+02\n",
      "   -2.2888e+04]\n",
      "  [ 0.0000e+00  8.0000e+00  8.0000e+00 ...  2.4600e+02 -3.4900e+02\n",
      "    2.3899e+04]\n",
      "  [-1.9968e+04  5.0000e+00  6.0000e+00 ... -6.2700e+02  1.5400e+02\n",
      "    3.1645e+04]\n",
      "  ...\n",
      "  [ 0.0000e+00 -3.0000e+00 -1.1000e+01 ... -2.8300e+02 -2.1100e+02\n",
      "    2.2375e+04]\n",
      "  [ 0.0000e+00  2.0000e+00  1.0000e+00 ...  4.0300e+02  1.1600e+02\n",
      "   -1.1785e+04]\n",
      "  [ 0.0000e+00  1.0000e+00  5.0000e+00 ...  8.1000e+01  7.1900e+02\n",
      "   -1.7227e+04]]]\n"
     ]
    }
   ],
   "source": [
    "print(datas.shape)\n",
    "print(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 将 NumPy 数组保存到磁盘\n",
    "np.save('PBS45p_q10_5000_SVD_60_80dB_ensemble1_Acq_Data.npy', datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 将 NumPy 数组保存到磁盘\n",
    "np.save(data_name+'.npy', datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 1216, 192)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 从磁盘加载 NumPy 数组\n",
    "data_array = np.load('PBS45p_q10.npy')  \n",
    "data_array = data_array.astype('float32')\n",
    "data_array = data_array.transpose(0 ,2, 1)\n",
    "print(data_array.shape)  # (5001, 192, 1216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1216, 192)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 从磁盘加载 NumPy 数组\n",
    "label_array = np.load('PBS45p_q10_5000_SVD_60_80dB_ensemble1.npy') \n",
    "label_array = label_array.astype('float32') \n",
    "label_array = label_array.transpose(0, 2, 1)\n",
    "print(label_array.shape)  # (5000, 192, 1216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader  #, TensorDataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_array, label_array, device):\n",
    "        self.data = torch.from_numpy(data_array).float().unsqueeze(1)#.to(device)\n",
    "        self.label = torch.from_numpy(label_array).float().unsqueeze(1)#.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "\n",
    "device = \"cuda\"\n",
    "seq_train, seq_val, seq_test = int(5000*0.7), int(5000*0.1), int(5000*0.2)\n",
    "# data_tensor = torch.from_numpy(data_array)#.half()\n",
    "# label_tensor = torch.from_numpy(label_array)#.half()\n",
    "# data_tensor = data_tensor.unsqueeze(1).to(device)\n",
    "# label_tensor = label_tensor.unsqueeze(1).to(device)\n",
    "train_dataset = CustomDataset(data_array[:seq_train], label_array[:seq_train], device)\n",
    "val_dataset = CustomDataset(data_array[seq_train:seq_train+seq_val], label_array[seq_train:seq_train+seq_val], device)\n",
    "test_dataset = CustomDataset(data_array[seq_train+seq_val:seq_train+seq_val+seq_test], label_array[seq_train+seq_val:seq_train+seq_val+seq_test], device)\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'einops'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlp_mixer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPMixer_inv\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# model = MLPMixer(\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     image_size = (1216, 192),\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     channels = 1,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#     num_classes = 1000\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m MLPMixer_inv(\n\u001b[0;32m     14\u001b[0m     image_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1216\u001b[39m, \u001b[38;5;241m192\u001b[39m),\n\u001b[0;32m     15\u001b[0m     channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Wang\\Desktop\\WHP\\mlp_mixer.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rearrange, Reduce\n\u001b[0;32m      5\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (x, x)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPreNormResidual\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'einops'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mlp_mixer import MLPMixer_inv\n",
    "\n",
    "# model = MLPMixer(\n",
    "#     image_size = (1216, 192),\n",
    "#     channels = 1,\n",
    "#     patch_size = 16,\n",
    "#     dim = 1216,\n",
    "#     depth = 6,\n",
    "#     num_classes = 1000\n",
    "# )\n",
    "\n",
    "model = MLPMixer_inv(\n",
    "    image_size = (1216, 192),\n",
    "    channels = 1,\n",
    "    patch_size = 16,\n",
    "    dim = 192,\n",
    "    depth = 12,\n",
    "    num_classes = 1000\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 1, 1216, 192)\n",
    "output_tensor = model(img) # (1, 1000)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (encoder1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (encoder2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (encoder3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (encoder4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv4): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder4): Sequential(\n",
      "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder3): Sequential(\n",
      "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder2): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder1): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (final_layer): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "torch.Size([1, 1, 1216, 192])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder (Downsampling path)\n",
    "        self.encoder1 = self.conv_block(1, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "\n",
    "        # Decoder (Upsampling path)\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self.conv_block(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        # Final output layer\n",
    "        self.final_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.maxpool(enc1))\n",
    "        enc3 = self.encoder3(self.maxpool(enc2))\n",
    "        enc4 = self.encoder4(self.maxpool(enc3))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.maxpool(enc4))\n",
    "\n",
    "        # Decoder path\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((enc4, dec4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((enc3, dec3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((enc2, dec2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((enc1, dec1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        # Final layer\n",
    "        return self.final_layer(dec1)\n",
    "\n",
    "    @property\n",
    "    def maxpool(self):\n",
    "        return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# Create the U-Net model\n",
    "model = UNet()\n",
    "print(model)\n",
    "\n",
    "# Example input\n",
    "x = torch.randn(1, 1, 1216, 192)\n",
    "output = model(x)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SymmetricCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymmetricCNN, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Output: (32, 192, 1216)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Output: (64, 192, 1216)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # Output: (128, 192, 1216)\n",
    "        \n",
    "        # Define the transposed convolutional layers (to upscale back to original size)\n",
    "        # self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)  # Output: (64, 192, 1216)\n",
    "        # self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)  # Output: (32, 192, 1216)\n",
    "        # self.deconv3 = nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1)  # Output: (1, 192, 1216)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply convolutions\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        \n",
    "        # Apply transposed convolutions\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        x = self.deconv3(x)  # No ReLU on the last layer to maintain original value range\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(192, 192*4)\n",
    "        self.linear2 = nn.Linear(192*4, 192)\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply convolutions\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "# Example usage:\n",
    "model = SymmetricCNN()\n",
    "input_tensor = torch.randn(1, 1, 192, 1216)  # Batch size 1, 1 channel, 192x1216 image\n",
    "output_tensor = model(input_tensor)\n",
    "print(output_tensor.shape)  # Should be torch.Size([1, 1, 192, 1216])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "output_dir = 'output_images'\n",
    "\n",
    "def img_process(image):\n",
    "    # image = image.transpose(1, 0)\n",
    "    # Convert numpy arrays to PIL Images\n",
    "    image = (image).astype(np.uint8)\n",
    "    pil_image = Image.fromarray(image)\n",
    "    return pil_image\n",
    "\n",
    "def save_img(batch_data, outputs, batch_labels=None, num=0, output_dir='output_images'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for j in range(batch_data.size(0)):\n",
    "        input_image = batch_data[j].detach().squeeze().cpu().numpy()\n",
    "        output_image = outputs[j].detach().squeeze().cpu().numpy()\n",
    "        \n",
    "        input_pil = img_process(input_image)\n",
    "        output_pil = img_process(output_image)\n",
    "        \n",
    "        # Save images\n",
    "        input_pil.save(os.path.join(output_dir, f'input_{num * batch_data.size(0) + j}.png'))\n",
    "        output_pil.save(os.path.join(output_dir, f'output_{num * batch_data.size(0) + j}.png'))\n",
    "\n",
    "        if batch_labels!=None:\n",
    "            label_image = batch_labels[j].squeeze().cpu().numpy()\n",
    "            label_pil = img_process(label_image)\n",
    "            label_pil.save(os.path.join(output_dir, f'label_{num * batch_data.size(0) + j}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/1, batch: 0, Loss: 178.57675170898438, Time:5.486355543136597\n",
      "epoch: 1/1, batch: 100, Loss: 155.3648681640625, Time:22.626109838485718\n",
      "epoch: 1/1, batch: 200, Loss: 151.6075897216797, Time:22.69310975074768\n",
      "epoch: 1/1, batch: 300, Loss: 156.5375518798828, Time:22.762123107910156\n",
      "epoch: 1/1, batch: 400, Loss: 133.0918731689453, Time:22.80213212966919\n",
      "epoch: 1/1, batch: 500, Loss: 104.75497436523438, Time:22.841140270233154\n",
      "epoch: 1/1, batch: 600, Loss: 85.1009750366211, Time:22.85864806175232\n",
      "epoch: 1/1, batch: 700, Loss: 79.78836059570312, Time:22.862650394439697\n",
      "epoch: 1/1, batch: 800, Loss: 70.32545471191406, Time:22.805132627487183\n",
      "Epoch 1/1, Loss: 118.13265032087054\n",
      "Val Loss: 71.27994116210938\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 1\n",
    "time_start = time.time()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(device)#.half()\n",
    "for epoch in range(num_epochs):\n",
    "    # train\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for i, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "        # print(batch_data.shape)\n",
    "        # print(batch_labels.shape)\n",
    "        batch_data = batch_data.to(device)#.float()\n",
    "        batch_labels = batch_labels.to(device)#.float()\n",
    "\n",
    "        out = model(batch_data)#.unsqueeze(1)\n",
    "        # out = model(batch_data).permute(0, 2, 1).unsqueeze(1)\n",
    "        # print(out.shape)\n",
    "        # print(batch_labels.shape)\n",
    "        loss = criterion(out, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch: {}/{}, batch: {}, Loss: {}, Time:{}\".format(epoch+1, num_epochs, i, loss.data, time.time()-time_start))\n",
    "            save_img(batch_data, out, batch_labels=batch_labels, num=i, output_dir='output_images_train')    \n",
    "            time_start = time.time()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_data, batch_labels) in enumerate(val_loader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            out = model(batch_data)#.unsqueeze(1)\n",
    "            # out = model(batch_data).permute(0, 2, 1).unsqueeze(1)\n",
    "            loss = criterion(out, batch_labels)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "    print(f'Val Loss: {val_loss/len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./UNet_50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "output_dir = 'output_images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# val\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (batch_data, batch_labels) in enumerate(train_data_loader):\n",
    "        for j in range(batch_data.size(0)):\n",
    "            input_image = batch_data[j].squeeze().cpu().numpy()\n",
    "            output_image = batch_labels[j].squeeze().cpu().numpy()\n",
    "\n",
    "            input_image = input_image.transpose(1, 0)\n",
    "            output_image = output_image.transpose(1, 0)\n",
    "            \n",
    "            # Convert numpy arrays to PIL Images\n",
    "            input_image = (input_image).astype(np.uint8)\n",
    "            output_image = (output_image).astype(np.uint8)\n",
    "            \n",
    "            input_pil = Image.fromarray(input_image)\n",
    "            output_pil = Image.fromarray(output_image)\n",
    "            \n",
    "            # Save images\n",
    "            input_pil.save(os.path.join(output_dir, f'input_{i * batch_data.size(0) + j}.png'))\n",
    "            output_pil.save(os.path.join(output_dir, f'output_{i * batch_data.size(0) + j}.png'))\n",
    "        if(i==10): break\n",
    "print(f'Test Loss: {test_loss/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.printInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 71.22007745361329\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "output_dir = 'output_images_test'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "outList = np.zeros([seq_test, 1216, 192])\n",
    "# test\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (batch_data, batch_labels) in enumerate(test_loader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        out = model(batch_data)\n",
    "        outList[i*batch_size:(i+1)*batch_size] = out.detach().squeeze().cpu().numpy()\n",
    "        loss = criterion(out.unsqueeze(1), batch_labels)\n",
    "        test_loss += loss.item()\n",
    "        save_img(batch_data, out, batch_labels=batch_labels, num=i, output_dir=output_dir)    \n",
    "    \n",
    "print(f'Test Loss: {test_loss/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (batch_data, batch_labels) in enumerate(test_loader):\n",
    "        out = model(batch_data)\n",
    "        loss = criterion(out, batch_labels)\n",
    "        test_loss += loss.item()\n",
    "    \n",
    "print(f'Test Loss: {test_loss/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
